const AppError = require("../utils/appError");
const catchAsync = require("../utils/catchAsync");

const imagedesc = {
  title: `"Person, Shoes, Tree. Is the Person Naked?" What People with Vision Impairments Want in Image Descriptions`,
  sections: [
    {
      sectionHeading: `Abstract`,
      sectionContent: [
        `Access to digital images is important to people who are blind or have low vision (BLV). Many contemporary image description efforts do not take into account this population’s nuanced image description preferences. In this paper, we present a qualitative study that provides insight into 28 BLV people’s experiences with descriptions of digital images from news websites, social networking sites/platforms, eCommerce websites, employment websites, online dating websites/platforms, productivity applications, and e-publications. Our findings reveal how image description preferences vary based on the source where digital images are encountered and the surrounding context. We provide recommendations for the development of next-generation image description technologies inspired by our empirical analysis.`,
      ],
      summary: {
        model1: {
          short: `Access to digital images is important to people who are blind or have low vision (BLV). Many contemporary image description efforts do not take into account this population’s nuanced image description preferences.`,
          medium: `Access to digital images is important to people who are blind or have low vision (BLV). Many contemporary image description efforts do not take into account this population’s nuanced image description preferences.`,
          long: `Access to digital images is important to people who are blind or have low vision (BLV). Our findings reveal how image description preferences vary based on the source where digital images are encountered and the surrounding context. We provide recommendations for the development of next-generation image description technologies inspired by our empirical analysis.`,
        },
        model2: {
          short: `Access to digital images is important to people who are blind or have low vision (BLV) Many contemporary image description efforts do not take into account this population's nuanced image description preferences. We provide recommendations for the development of next-generation image description technologies inspired by our empirical analysis.`,
          medium: `Access to digital images is important to people who are blind or have low vision (BLV) Many contemporary image description efforts do not take into account this population’s nuanced image description preferences. Study provides insight into 28 BLV people's experiences with descriptions of digital images from news websites, social networking sites/platforms, eCommerce websites, employment websites, online dating websites, productivity applications, and e-publications. We provide recommendations for the development of next-generation image description technologies inspired by our empirical analysis.`,
          long: `Access to digital images is important to people who are blind or have low vision (BLV) Many contemporary image description efforts do not take into account this population’s nuanced image description preferences. Study provides insight into 28 BLV people's experiences with descriptions of digital images from news websites, social networking sites/platforms, eCommerce websites, employment websites, online dating websites, productivity applications, and e-publications. We provide recommendations for the development of next-generation image description technologies inspired by our empirical analysis. Back to the page you came from, contact us at http://www.mailonlineonline.com/news/science-research-report/blivestories/blueblueblue-blindblindblindness/blueblindness.`,
        },
      },
    },
    {
      sectionHeading: `Introduction`,
      sectionContent: [
        `Digital images are plentiful across the media and information landscape. Towards enabling people who are blind or have low vision (BLV) to consume such content, a variety of efforts focus on the provision of alternative text (alt text) that is read through a screen reader. A screen reader is a software application that enables people who are BLV to read the text that is displayed on the computer screen with a speech synthesizer or Braille display. Alt text image descriptions are read off by a screen reader when a content author has followed recommended protocol, e.g. [13], and created an alt text attribute within a document or website’s source code.`,
        `Though provision of alt text is a best practice, most digital images lack descriptions. A 2017 study of popular websites in many categories (as ranked by alexa.com) found that between 20% and 35% of images lacked descriptions, and that many images that did contain alt text had extremely low-quality descriptions, such as the word "image" or a filename [17]. Images on social media are particularly problematic; a 2018 study found that only 0.1% of images on Twitter had alt text [16]. While the ideal is for content authors to always provide high quality image descriptions (i.e. using the alt text field) at the time of document authorship, many are not despite efforts and resources developed to scaffold content authors in producing them (e.g., [13, 26]).`,
        `The absence of alt text from content authors has motivated scholars and practitioners to innovate, by introducing a variety of more scalable image description services that are powered by humans [4, 5, 7, 6, 45], computers [14, 24, 35, 37, 38, 43], and a mixture of their efforts [17, 28, 32, 33]. In designing image descriptions, such services can leverage the many guidelines for how to write effective descriptions [13, 11, 26, 29, 30, 34, 39, 41, 42, 44]. However, existing guidelines are limited in that they do not clarify how to account for the finding of Petrie et al. [30] in 2005 – an interview study with five blind people that found that the most useful information to be included "was thought to be context dependent", i.e. based on the source in which the image is found.`,
        `Towards the goal of closing this description gap between what people want and what is provided, we present a qualitative study designed to investigate the image description preferences of people who are BLV. We interviewed 28 BLV people, guided by the question: "What are BLV people’s experiences with and preferences for image descriptions found in different digital sources?". We draw on the following definition of source: the platforms and media where one may encounter digital images. Examples of digital images found in different sources are shown in Figure 1. We focused our investigation on seven sources: news websites, social networking sites/platforms, eCommerce websites, employment websites, online dating websites/platforms, productivity applications, and e-publications. We conclude with recommendations regarding what is important information to incorporate into image descriptions found in different sources. These recommendations can be of great value for improving human-powered, computer-powered, and hybrid image description services for people who are BLV. More generally, our work contributes to the design of social and technical infrastructures that are accessible to all and support people to engage more fully with digital media.`,
      ],
      summary: {
        model1: {
          short: `Digital images are plentiful across the media and information landscape. A 2017 study of popular websites in many categories (as ranked by alexa.com) found that between 20% and 35% of images lacked descriptions, and that many images that did contain alt text had extremely low-quality descriptions, such as the word "image" or a filename [17]. In designing image descriptions, such services can leverage the many guidelines for how to write effective descriptions [13, 11, 26, 29, 30, 34, 39, 41, 42, 44]. Towards the goal of closing this description gap between what people want and what is provided, we present a qualitative study designed to investigate the image description preferences of people who are BLV.`,
          medium: `Digital images are plentiful across the media and information landscape. Though provision of alt text is a best practice, most digital images lack descriptions. A 2017 study of popular websites in many categories (as ranked by alexa.com) found that between 20% and 35% of images lacked descriptions, and that many images that did contain alt text had extremely low-quality descriptions, such as the word "image" or a filename [17]. In designing image descriptions, such services can leverage the many guidelines for how to write effective descriptions [13, 11, 26, 29, 30, 34, 39, 41, 42, 44]. However, existing guidelines are limited in that they do not clarify how to account for the finding of Petrie et al. [ Towards the goal of closing this description gap between what people want and what is provided, we present a qualitative study designed to investigate the image description preferences of people who are BLV. Examples of digital images found in different sources are shown in Figure 1. These recommendations can be of great value for improving human-powered, computer-powered, and hybrid image description services for people who are BLV.`,
          long: `Digital images are plentiful across the media and information landscape. Towards enabling people who are blind or have low vision (BLV) to consume such content, a variety of efforts focus on the provision of alternative text (alt text) that is read through a screen reader. Though provision of alt text is a best practice, most digital images lack descriptions. A 2017 study of popular websites in many categories (as ranked by alexa.com) found that between 20% and 35% of images lacked descriptions, and that many images that did contain alt text had extremely low-quality descriptions, such as the word "image" or a filename [17]. In designing image descriptions, such services can leverage the many guidelines for how to write effective descriptions [13, 11, 26, 29, 30, 34, 39, 41, 42, 44]. However, existing guidelines are limited in that they do not clarify how to account for the finding of Petrie et al. [ We interviewed 28 BLV people, guided by the question: "What are BLV people’s experiences with and preferences for image descriptions found in different digital sources?". We draw on the following definition of source: the platforms and media where one may encounter digital images. Examples of digital images found in different sources are shown in Figure 1. We focused our investigation on seven sources: news websites, social networking sites/platforms, eCommerce websites, employment websites, online dating websites/platforms, productivity applications, and e-publications. We conclude with recommendations regarding what is important information to incorporate into image descriptions found in different sources. These recommendations can be of great value for improving human-powered, computer-powered, and hybrid image description services for people who are BLV.`,
        },
        model2: {
          short: `A study of popular websites in many categories found that between 20% and 35% of images lacked descriptions, such as the word "image" or a file. Images on social media are particularly problematic; a 2018 study found that only 0.1% of. images on Twitter had alt text. We present a qualitative study designed to investigate the image description preferences of people who are blind.`,
          medium: `A study of popular websites in many categories found that between 20% and 35% of images lacked descriptions, such as the word "image" or a file. Images on social media are particularly problematic; a 2018 study found that only 0.1% of. images on Twitter had alt text. We present a qualitative study designed to investigate the image description preferences of people who are BLV. We conclude with recommendations regarding what is important information to incorporate into image descriptions found in different sources. These recommendations can be of great value for improving human-powered, computer-powered and hybrid image description services for people.`,
          long: `A study of popular websites in many categories found that between 20% and 35% of images lacked descriptions, such as the word "image" or a file. Images on social media are particularly problematic; a 2018 study found that only 0.1% of. images on Twitter had alt text. We present a qualitative study designed to investigate the image description preferences of people who are BLV. We conclude with recommendations regarding what is important information to incorporate into image descriptions found in different sources. These recommendations can be of great value for improving human-powered, computer-powered and hybrid image description services for people who have low vision (BLV) to people who want to consume digital media. The most useful information to be included is based on the source in which the image is found.`,
        },
      },
    },
    {
      sectionHeading: `Related work`,
      sectionContent: [
        `Our research builds on prior work including guidelines for alt text image descriptions, studies about BLV users’ image description preferences, and systems for facilitating or automating image description. Of importance, throughout this paper we use the term description as opposed to caption or alt text. Though the terms alt text and caption are commonly used in related scholarship, they infer specific linguistic structures of description that does not take into account contemporary AI-powered approaches to description as described in [28, 35].`,
        `Guidelines for Describing Images to People Who are BLV 
The task of creating image descriptions–interpreting visual information and transmuting its meaning into language–is non-trivial [20, 23, 26]. Still, numerous efforts have made authoring image descriptions more approachable. Many focus on guiding web developers [41]. For instance, the Web Content Accessibility Guidelines (WCAG) provide basic instructions for the generation of alt text. The Diagram Center [11] provides instruction on assessing whether images are functional or decorative, whether information can be gathered from surrounding text, and to provide age-appropriate descriptions. The Diagram Center also notes that effective image captions describe foreground, background, color, and directional orientation of objects [11]. Such suggestions are in line with findings from related scholarship [34].`,
        `While the aforementioned works focus on one-size-fits-all guidelines for authoring image descriptions, other efforts have noted that descriptions need to be responsive to the context in which an image is found. Petrie et al. (2005) championed this idea [30], albeit did not present findings according to individual source types. Rather, they recommended guidelines that represented description preferences commonly observed across 10 sources (10 homepages in 10 different sectors), which were that descriptions include 1) the purpose of the image, 2) what objects and people are present, 3) any activities in progress, 4) the location, 5) colors, and 6) emotion [30]. More recently, researchers have discussed the types of content that should be included in descriptions of images found on social networking sites (SNS): describe all salient objects [29]; specify who is in the image, where it was taken, and others’ responses to it [39]; indicate key visual elements, people, and photo quality [44]; and when captioning people, objects, and settings, specify details including the people count, facial expression, age, inside, outdoor, nature, close-up, and selfie [42]. Our work extends these prior works by identifying preferences of people who are BLV across seven sources. Building upon our observations, we also propose recommendations for the types of content that image description technologies should deliver for people who are BLV.`,
        `Understanding Users’ Experiences with Descriptions 
Our work relates to the body of literature aimed at understanding how people who are BLV experience image descriptions provided by technologies. The literature shows that people who are BLV want descriptions for digital images found on websites [30], on SNS [2, 29, 39], within digital publications, and in productivity applications [15]. Like many, they place value in image descriptions to stay up to date with the news [29], to enjoy entertainment media [29], and to engage in social interactions [2, 10, 29, 39, 44]. In addition to these common uses of images, people who are BLV depend on image descriptions to avoid risk (by not sharing images deemed unprofessional or low quality, or images that contain inappropriate content) [2, 8, 44]. In addition, scholars have found that people who are BLV want descriptions for images that they take in order to learn about the content of these images [4, 44]. `,
        `While the need for image descriptions is clear, few prior studies focus on understanding BLV people’s preferences for what kind of content they want described for images found on different sources. Our current understanding comes from a small body of dispersed literature. As previously noted, in 2005 Petrie et al. asked five BLV participants about the kinds of images they wanted described, what image content they wanted described, and their preferred length of description [30]. Others focused on BLV participants’ experience with descriptions for images presented on social media platforms and how BLV users perceive automatically generated captions [25, 43, 44]. Finally, others have inquired into how people who are BLV want to interact with image descriptions, and how different delivery structures impact their experience [28, 35, 40]. Despite the importance of these findings, to our knowledge no prior work has explored how BLV people’s preferences for the content in the image descriptions vary based on where they encounter an image description (e.g. on a social media site versus in an e-textbook). Our work fills this gap towards supporting opportunities to make image descriptions context-specific.`,
        `Image Description Technologies 
Many images found on digital sources do not contain alt text or effective image descriptions [17, 16]. The low rate of manually-produced descriptions has inspired some investigations into new approaches to generate image descriptions. These approaches are often described as human-powered, automated, and hybrid approaches. Human-powered approaches [5] provide near-real-time descriptions of images through crowdsourcing [4, 45], friendsourcing [7], and social microvolunteering [6]. Automated image description approaches employ artificial intelligence models to generate image descriptions [24, 14, 38, 37, 43, 35]. Hybrid image description technologies create human-in-the-loop workflows that work in tandem with automated approaches [17, 28, 32, 33]. Tools also have been introduced to train non-specialists (including crowdworkers) to identify which images and diagrams in text books need alt text [12, 26]. Extending prior work, our study reveals new design opportunities for improving image description technologies by contextualizing descriptions based on the source where images are found.`,
      ],
      summary: {
        model1: {
          short: `Our research builds on prior work including guidelines for alt text image descriptions, studies about BLV users’ image description preferences, and systems for facilitating or automating image description. For instance, the Web Content Accessibility Guidelines (WCAG) provide basic instructions for the generation of alt text. Such suggestions are in line with findings from related scholarship [34]. Understanding Users’ Experiences with Descriptions 
`,
          medium: `Our research builds on prior work including guidelines for alt text image descriptions, studies about BLV users’ image description preferences, and systems for facilitating or automating image description. Of importance, throughout this paper we use the term description as opposed to caption or alt text. Though the terms alt text and caption are commonly used in related scholarship, they infer specific linguistic structures of description that does not take into account contemporary AI-powered approaches to description as described in [28, 35]. Still, numerous efforts have made authoring image descriptions more approachable. Many focus on guiding web developers [41]. For instance, the Web Content Accessibility Guidelines (WCAG) provide basic instructions for the generation of alt text. While the aforementioned works focus on one-size-fits-all guidelines for authoring image descriptions, other efforts have noted that descriptions need to be responsive to the context in which an image is found. Building upon our observations, we also propose recommendations for the types of content that image description technologies should deliver for people who are BLV. Understanding Users’ Experiences with Descriptions 
`,
          long: `Our research builds on prior work including guidelines for alt text image descriptions, studies about BLV users’ image description preferences, and systems for facilitating or automating image description. Of importance, throughout this paper we use the term description as opposed to caption or alt text. Though the terms alt text and caption are commonly used in related scholarship, they infer specific linguistic structures of description that does not take into account contemporary AI-powered approaches to description as described in [28, 35]. Still, numerous efforts have made authoring image descriptions more approachable. Many focus on guiding web developers [41]. For instance, the Web Content Accessibility Guidelines (WCAG) provide basic instructions for the generation of alt text. Such suggestions are in line with findings from related scholarship [34]. While the aforementioned works focus on one-size-fits-all guidelines for authoring image descriptions, other efforts have noted that descriptions need to be responsive to the context in which an image is found. More recently, researchers have discussed the types of content that should be included in descriptions of images found on social networking sites (SNS): describe all salient objects [29]; specify who is in the image, where it was taken, and others’ responses to it [39]; indicate key visual elements, people, and photo quality [44]; and when captioning people, objects, and settings, specify details including the people count, facial expression, age, inside, outdoor, nature, close-up, and selfie [42]. Our work extends these prior works by identifying preferences of people who are BLV across seven sources. Building upon our observations, we also propose recommendations for the types of content that image description technologies should deliver for people who are BLV. Understanding Users’ Experiences with Descriptions 
`,
        },
        model2: {
          short: `The task of creating image descriptions–interpreting visual information and transmuting its meaning into language–is non-trivial. People who are BLV place value in image descriptions to stay up to date with the news, enjoy entertainment media, and engage in social interactions. Our work builds on prior work including guidelines for alt text image descriptions.`,
          medium: `The task of creating image descriptions–interpreting visual information and transmuting its meaning into language–is non-trivial. People who are BLV place value in image descriptions to stay up to date with the news, enjoy entertainment media, and engage in social interactions. Our work builds on prior work including guidelines for alt text image descriptions, studies about BLV users’ image description preferences, and systems for facilitating or automating image description. We use the term description as opposed to caption or alt text.`,
          long: `The task of creating image descriptions–interpreting visual information and transmuting its meaning into language–is non-trivial. People who are BLV place value in image descriptions to stay up to date with the news, enjoy entertainment media, and engage in social interactions. Our work builds on prior work including guidelines for alt text image descriptions, studies about BLV users’ image description preferences, and systems for facilitating or automating image description. We use the term description as opposed to caption or alt text, though the terms alt text and caption are commonly used in related scholarship, but they infer specific linguistic structures of description that does not take into account contemporary AI-powered approaches to description as described in [28, 35]. Our work fills this gap towards supporting image descriptions`,
        },
      },
    },
    {
      sectionHeading: `Study design`,
      sectionContent: [
        `We conducted a qualitative study guided by the following two research questions: 
RQ1: What are BLV people’s experiences with digital images on different sources? 
RQ2: What are BLV people’s description preferences for digital images in these different sources? 
We formed these questions based on the understanding that source is a significant factor that impacts a person’s description preferences [30]. We assumed this to be the case in order to limit the scope of this study.`,
        `Data Collection 
To learn about BLV people’s experiences with digital images that they encounter on different sources, we designed a semistructured interview protocol that included 15 open-ended questions, 13 Likert survey statements, and a contextual inquiry. Prior to each interview, we asked each participant to bring their preferred access technology with them. We audio recorded each interview. After the interviews, we sent the audio files to be transcribed by a professional service. We also took field notes to keep track of emerging themes.`,
        `Our interview procedures are in the Supplementary Materials. In summary, for the open-ended research questions, we asked about our participants’ visual impairment, access technology preferences, experience with digital images, and experience with technologies and services that provide image description (from alt text to automated image description services). For the contextual inquiry, we asked participants to open their technology and visit three to five sources where they would expect to find digital images; the number of sources varied based on how long it took for the participant to complete the task or their familiarity and interest in the source. We suggested the following options: a news website, a SNS post, an eCommerce website, an organization or employment web page, and a productivity document, e.g. Word or PowerPoint. We identified these sources based on prior work that indicated that people who are BLV want image descriptions to pursue their interests through staying up to date with the news [29], enjoying entertainment media [29], eCommerce [35], staying socially connected [39, 29, 2, 44, 10], dating [31] and performing work or academic pursuits [15].`,
        `Participant Recruitment 
We recruited participants by circulating an IRB-approved announcement on social media, on a listserv managed by orga-nizations serving people who have visual impairments, and through snowball sampling at an independence training cen-ter. To be eligible, we specified that participants had to be at least 18 years old, be BLV, and use a screen reader and/or magnification. The announcement explained that participants would be compensated with an Amazon gift card, at the rate of 20 USD per hour. We aimed to have equal participation of people who have congenital blindness, acquired blindness, congenital low vision, and acquired low vision. At the onset of recruitment, we accepted all participants that met our basic criteria for inclusion. After 20 participants were recruited, we selected participants based on their visual impairment towards the goal of equal representation.`,
        `In total, 28 people participated in our study. We conducted 25 of the interviews in person in a 50 mile radius of our U.S. metro area, and another 3 over the phone with individuals in other states to achieve greater diversity of visual experience within our participant pool. The same protocol was used when conducting the interview over the phone, with the key dis-tinction being that the researcher and the participant accessed the image sources on their own devices when conducting the contextual inquiries. We believe that we reached participant saturation based on Alroobaea et al.’s finding, which states that there is no certain number of participants for finding all usabil-ity problems (during interviews and think-aloud approaches), though the rule of 16+/-4 users gains much validity in user testing [1]. The interviews lasted between 1.25 hours and 2 hours, depending on the participant’s experiences and interest in the topic. All participants used Apple or Android phones for the contextual inquiries.`,
        `Table 1 summarizes participants’ demographic information. As shown, the participants represent a diversity of backgrounds in terms of gender (16 women, 12 men), age (range is 18 to 67 with a mean of 39.05), education (from people who had not completed high school to people who have a doctorate), and occupation (from people who are unemployed or retired, to those who are students, DJs, lawyers, and educators). These participants had a range of visual impairments (from unformed retinas, to myopia, to blindness acquired due to laser surgery) as well as varied experiences with visual information.`,
        `Data Analysis 
After conducting all 28 interviews, we performed a qualitative analysis of the transcribed data. We then performed axial coding, a process of identifying and relating codes to each other, via a combination of inductive and deductive thinking [36]. We used deductive reasoning to identify the sources of interest based on the literature, and then inductive reasoning to attribute the content patterns to these sources. To prepare the data for the axial coding, two team members cleaned up major errors in the transcript by reviewing the audio, all the while taking analytical memos to record emergent themes.`,
        `At the onset we established the seven sources (news websites, social networking sites/platforms, eCommerce websites, employment websites, online dating websites/platforms, productivity applications, and e-publications) plus an other category to account for emergent sources as parent codes (or primary phenomena orienting the study). We then used a semantic analysis technique to identify and code text segments according to the parent codes. Braun and Clark explain that to perform semantic analysis one should "not [be] looking for anything beyond what a participant has said or what has been written [9]." While doing this, we dynamically identified and refined a set of child codes. Child codes that we identified include: Image Access Behavior: statements about how one approaches consuming the media; Image Access Experience: statements related to one’s exposure or interaction with content in digital images; Description Content Wants: statements about the features, attributes, or details that should be included in an image description, and Description Considerations: statements related to the factors that impact image access or content preferences. Within each subset of data we made note of common and unique themes amongst all participants’ responses. For instance, under the child code Description Content Wants we noticed often times participants talked about the level of detail they wanted for an image on a source or their need to understand the purpose of an image.`,
        `Performing the qualitative data analysis with the sources as the parent codes enabled us to perform a cross-source analysis that highlights how our participants’ image experiences and description content preferences differ based on source. Of note, we present the variety of perspectives shared by our participants, as opposed to a quantitative analysis of how many people in our participant group shared the same experience, because the aim of this work was to understand the range of experiences and content wants as opposed to the frequency.`,
      ],
      summary: {
        model1: {
          short: `We conducted a qualitative study guided by the following two research questions: 
`,
          medium: `We conducted a qualitative study guided by the following two research questions: 
`,
          long: `We conducted a qualitative study guided by the following two research questions: 
`,
        },
        model2: {
          short: `We designed a semistructured interview protocol that included 15 open-ended questions, 13 Likert survey statements, and a contextual inquiry. 28 people participated in our study. Participants were recruited by circulating an IRB-approved announcement on social media, on a listserv managed by orga-nizations serving people who have visual impairments.`,
          medium: `We designed a semistructured interview protocol that included 15 open-ended questions, 13 Likert survey statements, and a contextual inquiry. 28 people participated in our study. Participants were recruited by circulating an IRB-approved announcement on social media, on a listserv managed by orga-nizations serving people who have visual impairments, and through snowball sampling at an independence training cen-ter. The interviews lasted between 1.25 hours and 2 hours, depending on participant’s experiences and interest in the topic.`,
          long: `We designed a semistructured interview protocol that included 15 open-ended questions, 13 Likert survey statements, and a contextual inquiry. 28 people participated in our study. Participants were recruited by circulating an IRB-approved announcement on social media, on a listserv managed by orga-nizations serving people who have visual impairments, and through snowball sampling at an independence training cen-ter. The interviews lasted between 1.25 hours and 2 hours, depending on the participant’s experiences and interest in the topic. All participants used Apple or Android phones for the contextual inquiries. We used deductive reasoning and deductive thinking to identify the sources of interest based on the content of interest. We then performed a laser-based analysis of the data.`,
        },
      },
    },
    {
      sectionHeading: `Findings`,
      sectionContent: [
        `FINDINGS 
We present our findings for our research questions: RQ1: What are BLV people’s experiences with digital images on different sources? RQ2: What are BLV people’s description preferences for digital images in these different sources? 
Source 1: News Articles 
RQ1: While many participants shared that they read news articles, we observed in the contextual interviews that none of the image descriptions encountered provided the participants with the information they needed in order to understand what was in the image. One reason was because the alt text was uninformative; e.g., it simply states ".jpeg" or a long file name. Participants’ responses to uninformative alt text was similar to the sentiment shared by P28: "I don’t know what the heck it was, but I’m sure the article will tell me what the image was." Reinforcing this perspective, we heard P26 share what she thought was in the image based on the article headers: ”Honestly, I don’t really know. Okay. Okay. They’re explaining there, going into all these, this detail about like one of these explosions are happening on the earth’s inner core and that gives me like no information on what’s going on in the image.” Another common reason participants did not engage with images was that they were unaware an image was present on the web page, but some participants acknowledged this could be happening. In P17’s words, "That’s annoying. I’m sorry. So the way that this website is structured is unclear. It has a heading that shows the title of the article and then it has a bunch of other headings to other news articles. So it’s difficult to tell. There is no image as far as I can tell."`,
        `RQ2: Participants shared that they want image descriptions that clarify the purpose of the image in the news sources. As P28 noted, "So usually if there is an image attached to an article, there’s a reason for that image. They may take 1500 pictures of a protest, but only choose two [to] be on the website. Why did those two pictures get chosen?" In P16’s words, "I think it’s [images are] just information to tell the story. But, just saying ’image’ does nothing. If there’s an image, tell me why it’s important, I guess."`,
        `Regarding the type of description content that participants want, we heard a variety of preferences based on the news story. For example, P05 noted, "[My preferences] depend on the article, but I would say the scene [of the event]. What like is it a politician in the, like a person of interest? Um, is it a sports team? What are they doing? So like what’s the scene, what’s happening, what are the actions?" This variety reinforces participants’ stated preference for descriptions to clarify the purpose of images. We elaborate about some of the variability we heard below.`,
        `For images where people are the central focus, the key content they want centers on the identifiable characteristics of the person or group and important details about their interactions or experiences. As P19 shared, "There are times when all you want to know is that there’s a group of people sitting on a bus, other times you want to know that all the people are all smiling or had tears rolling down their faces and they look really sad– especially in like any kind of a news article." In addition, we observed that in some cases our participants want to know about a person’s race or ethnicity. As P9 noted, "I would put that in there to make sure that everybody is represented equally. For me, it helps keep the news accountable and aware of their racial profiling." Others noted that knowing whether a person is a celebrity or not matters within the news context because they have cultural influence.`,
        `For images where the focus is on the events and scenes, participants want to know about the central people or objects, the activity that they are engaged in, how they are interacting with one another, and pertinent information about the setting.`,
        `For images that highlight objects or landmarks, they want to hear about the unique characteristics or features. As P4 noted, "If I was reading about a new airplane I might want to know how big it is, how many people it carries." As P11 pointed out "sometimes you want to know what is written on the protest sign behind the person standing at the podium."`,
        `Source 2: Social Networking Sites (SNS) 
RQ1: Most participants shared that they use SNS. While participants reported a high engagement with image descriptions in SNS, this only was with respect to Facebook. As P08 shared, "In comparison to other places, Facebook app is honestly pretty accessible. If there’s someone in the images it will tell me the name of the person and something about the setting." Despite the positive responses, participants also readily described limitations. For example, P02 shared, "It’s very hit or miss. Sometimes it’ll just tell me ‘person, shoes, and trees.’ Is the person naked? Does that mean that there’s a person only wearing shoes and they’re standing on top of a tree? It’s not specific enough content." Participants reported frustrations with other SNS that do not provide image descriptions. For example, P07 noted, "Twitter–they are not accessible at all. I think that Facebook gives their images alt text but Twitter does not." Similarly, P14 shared, "I do use Twitter once in a while. Last time I was there I noticed the pictures and images alt text weren’t there."`,
        `RQ2: Participants shared that they want image descriptions in SNS that help them understand the purpose of the image. As P16 noted, "People share a lot of personal images. You have to infer why they’re sharing it based on their strange texts. More detail is necessary." We learned that purpose is especially important when the person posting the image does not provide a comment or the comment did not directly reference the image content.`,
        `For images on Facebook, the type of description content our participants wanted centered on descriptions of people. For instance, P09 noted, "I like to have them include more like facial expression...were they smiling or smirking? Was it a mischievous look?" More generally, participants want to recognize facial expressions or body language to help them decide how to respond to the image. Notably, participants want more content described when they or people they know were in the image. As P12 shared, "I want it pretty detailed especially if it’s somebody I know...what’s going on and why they’re in the picture and what else is happening in the picture."`,
        `Other description wants center on the elements in an image that help them understand what the person is doing or their environment. In the context describing a family portrait, P05 asked for "Something like five family members standing in front of a Christmas tree or something like that. The number of people and who’s in the picture. Who’s in the picture and their actions–what they’re doing." In a different context, P09 shared, "If my friend was showing off her engagement ring...I would want to know if it was a princess cut. Just giving that it is a ring...that is not enough." Additionally, if a person’s attire is remarkable, our participants wanted that information.`,
        `Source 3: eCommerce Apps and Websites 
RQ1: Many participants indicated that they shop online. Amazon was the primary website of choice. Participants shared that they shop online for clothing, household items, electronics, entertainment media, gift items, as well as to do research about new products and as a hobby.`,
        `Overall, we learned that our participants have very low expectations for image descriptions on eCommerce sources. We repetitively heard frustration and apathy from many participants. For example, P2 exclaimed, "Amazon gives really poor descriptions honestly...I mean it really is all you get is Ding, Ding, Ding, Ding, Ding [the screen reader issuing a tone for an empty image description]. Amazon can really piss me off. I was buying an ottoman. There was no description in a picture. I had to lurk in the comments looking for somebody who finally said what the hell it looked like. They encourage people to put photos in the comment section; none of those photos are described." P19 noted, "Amazon...So one kind of pet peeve I have of pictures is that a lot of times since there isn’t any alt texts or anything, some of the screen readers will tend to think that the path of the picture should be read. So you’ll have this entire, 5,000 character long path name...you have to read a page of these stupid identifiers." In addition to the frustration for missing information or nonsensical descriptions, our participants expressed concern that they do not have equal access to image content on eCommerce websites. P5 shared, "Amazon is the least accessible. Accessibility for me, it means being able to get the information from an image comparable to how a sighted person would get that information. I don’t get that."`,
        `RQ2: The type of description content participants want for eCommerce centers on descriptions of objects. This is unsurprising given that many images on eCommerce sources contain one product on a clean, solid-colored background. The specific descriptive details participants wanted varied based on the type of product.`,
        `For clothing, they first wanted to know color and then attributes such as the general style of the garment (formal, professional, athletic, casual), stylistic details on the garment (zippers, pockets, thick hemlines, sleeve length, material, pattern), and how it fits on the model’s body as well as the model’s body shape. For example, P6 noted, "Color is interesting, so is the length of the sleeves. Maybe the cut, zip front, the hem line, how it fits, are the selves big or tight? Does it have drawstrings?" P9 shared, "I would say the model is the model really skinny? Is the model more of a plus size is, because to me the models really help paint the picture of how the shirt [is] gonna fit."`,
        `When it came to household items and electronics, participants’ description wants centered on the unique attributes of the object’s form or materials, as well as text, symbols, or logos on the item. For instance, when P18 was learning about an image of a mug, she asked very specific questions like "How much of the cup is covered by it [the pattern]? How and where does the handle attached? Is there anything about it that makes it look good to use while traveling?" When discussing purchasing items on eBay, P8 reflected, "It would be great if they described any scratches or dents or cracks on an item." The participants also want any text or logos described for products. For example, P19 brought up a picture of a computer adapter with a lightening bolt port and had the interviewer describe the picture to him. He responded, "I didn’t know that the lightning bolt was actually a picture of a lightning bolt on it [the computer adapter]. I definitely want that detail".`,
        `Source 4: Employers’/Employment Websites
RQ1: Our participants’ familiarity with employment websites varied from current and active use to no familiarity. For some people, an employment website meant a specific employer’s web page, for others it meant a potential client’s website, for others it meant job boards (USAJobs or Indeed). None of our participants recalled encountering image descriptions on employment websites, job boards, or the like. Several people were surprised that we would ask about this source. As P02 shared, "I feel like you’re on level 5,000; I’m still trying to figure out if there is a picture on any page. Am I missing functional content or is this just like decorative? I just assumed that all the images are not described on those job sites because they are decorative."`,
        `RQ2: For type of description content, participants primarily want to learn about people in the images and the work environment. Most prominently, participants want to know the facial expressions. As P23 put it, "If they all look like they’re miserable, you’re probably don’t want to work there or help them." Participants also want to learning about people’s attire; As P05 explained, "I want to know how a person is dressed and looking...first impression is important in the music industry; you judge people by how they’re dressed." Some want content that would help them learn about the diversity of the people working at the company. As P10 shared, "If there are photos of board members, I want to know if they were a bunch of white guys, if there is racial diversity." Others expressed interest in getting information about the types of work tasks people engaged in and the work setting. P21, "Whether the office looks busy. Are they sitting around or at a desk? Is it like a party that they’re having?" P25 anticipated wanting to know, "Is it cluttered? User-friendly? Does it have dark walls, light walls? That might not be directly relevant to me, but it will give me a lot of information about the overall work environment and people’s attitudes."`,
        `Source 5: Online Dating Websites/Applications 
RQ1: All participants reported they had never visited an on-line dating website. Additionally, none provided the name of a dating website and none suggested going to a dating website during the contextual interview. The reasons reported for not using them centered on the sources’ overall inaccessibility, that it is preferable to meet others in person, or that they were not in need. This said, all participants provided meaningful answers to our questions about their description preferences and expressed interest in this source.`,
        `RQ2: The types of description content participants want centers on describing physical characteristics of a person, with specific interest in the color of a person’s hair, the style of their hair (and/or the style of a man’s facial hair, if applicable), the body type, and/or weight. Some people indicated that they would want to know somebody’s eye color, race or skin tone, facial expression, and/or if the person had a defining physical characteristic. For instance, P03 noted, "I’d say that would probably be the one defining feature. Like in any extreme irregular irregularities... things that are obvious to a person that is sighted." Other attributes that emerged as important include: the person’s attire, how well kept or clean they appear, as well as the presence of any tattoos (and what they depict). Some of our participants also indicated wanting details about the setting of the photo "because that gives me information about their interests" (P25). Other content wants centered on knowing whether a pet is in the image, and the composition of the photo, e.g. whether all of the photos were selfies, candid photos, or of larger group shots to know more about how subjects presented themselves.`,
        `Our participants noted that if they knew the person describing the image to them, they would be more inclined to ask for a subjective evaluation of the way a person looked. Importantly, we heard a variety of concerns related to whether an image description of a person can be objective or unbiased. For instance, P02 noted, "How do you really describe a person? Isn’t that judgment call? Even if it is as objective as you can, there’s still going to be different things that people like [...] That [diversity] should be the beauty as opposed to losing it to norms." P10 shared, "I don’t want a third party telling me they think someone is handsome or beautiful." P16, "It’s going to be very subjective. I mean I guess you could comment on some things that are not a judgement."`,
        `Source 6: Productivity Applications 
RQ1: Participants’ engagement with productivity applications varied greatly.7 For those who use them, they reported low engagement with image descriptions. Comments about this include "I have encountered images in only a few cases, but I don’t feel like it was intentional. I’m going to say they’re almost nonexistent because they’re not there" (P03), "I don’t encounter images on Word. As far as I know, no one sent me anything in Word that had a picture" (P02), and "I use PowerPoint, but I have to have help. I basically create slides and then have somebody help me find and paste pictures in. But reading PowerPoints is even worse. When a professor gives me them, they’re not described." (P16). Still, several participants reported using features to add alt text to images. As P08 shared, "PowerPoint has started doing this thing where when you create a PowerPoint, you can actually go into the settings and put alt texts on the images and I love it!"`,
        `RQ2: For text editing documents, the primary concern we heard centers on whether an image is decorative or functional. For example, P11 noted, "If it’s just like a placeholder image that is not relevant to the text, I don’t really care if it’s described." P10 shared, "Hopefully I can figure it out if it’s something that would be important, and then I can figure what to pay attention to."`,
        `The type of content participants want varies based on the image’s purpose. In the words of P09 "It would depend on the context of the document and what it was about. I would want enough information to give myself as close of a representation of that experience as I could to recreate that." In reference to an image on a PowerPoint presentation, P11 noted, "If it’s like a biology presentation or document about a molecule and there’s a picture of a molecule, I want to know like what does the molecule look like, like what are the bonds and the atoms and stuff like that."`,
        `Source 7: E-Publications 
RQ1: The participants in our study had a range of experiences using e-publications, which they understood to be digital textbooks, PDFs, and materials found on audio book platforms like Bard and Bookshare. The participants who had experience using digital textbooks noted that the images presented within this source were not accessible to them. P08 noted, "Last year I used an online textbook. They didn’t have any way of describing for you what pictures actually were." P17 shared, "When I came across an image it would just say image." P28 expressed, "The problem is that if there are images that are often not described, so this is particularly unhelpful." The participants who mentioned encountering and/or using PDFs shared that digital images within this format are almost always inaccessible. P06 noted, "Occasionally I might get an email with a PDF attachment. The images in them are mostly not accessible." P13 shared, "PDFs read funny a lot." The participants who mentioned accessing materials through audio output (e.g. Bard, Bookshare) reported a similar dearth of images being described. Importantly, when speaking to participants about digital images in e-publications, they often spoke about diagrams, charts, or maps.`,
        `RQ2: In terms of type of description content, many participants simply said "same as for Productivity Applications" or "it depends on the context of the image." The lack of depth in their answers may be attributed to the fact that we did not vary the order of our questions for each participant (i.e. since this category was last, participants may have been fatigued) or that participants had less experience with images in this source.`,
        `Emergent Sources 
While we designed our study to focus on seven sources, additional sources of interest emerged. We describe these below. 
Web Browsers: Participants reported encountering inaccessible images when searching on web browsers. An example is advertisements that pop-up when searching. While P10 noted that advertisements are generally a nuisance, he also shared "I’d want the option to have it described."`,
        `Instructional Websites/Blogs: Participants reported encountering images on blogs or websites that contain instructions for how to accomplish a task, such as crafting or cooking. The expressed description wants focused on details about the objects being made, and if there "are more than one picture, what difference there is between the photos so I can follow along with the instructions."`,
        `Hotel websites: Participants reported coming across inaccessible images on hotel websites. One participant (P10) provided a list of his content interests, "[For example, in an image description I would want something like], ‘Our front desk clerk stands behind the podium so they can step out easily to work directly with someone in a wheelchair. Our lobbies are covered in plush carpets, or we have tactile different floor surfaces’."`,
        `Personal Photo Gallery: Participants wanted descriptions for images they had taken to help them know what they were sharing with friends or to organize their albums. These participants did not provide explicit description wants; we attribute this omission to us not speaking about a specific purpose for the images.`,
        `Public GUIs: Participants noted they encounter digital images on public devices or interfaces in libraries or airports, but did not specify content they would want in descriptions.`,
        `Cross-Source Analysis Level of Experience with Digital Images: During our analysis of the data with respect to RQ1, "What are BLV people’s experiences with digital images on different sources?", we observed that people who are BLV generally have low engagement with digital images. In some instances the low level of engagement was linked to their familiarity and use of such sources. For instance, none of our participants had direct experience with images on online dating websites as a factor of not using them. When discussing images found on employment websites and productivity applications we learned that it may be difficult for people who are BLV to discern whether an image is present, in part because they do not use these sources as often as others and/or that they do not anticipate a strong purpose for the images on these sources. For other sources, low engagement stemmed from inadequate descriptions of images on the websites (e.g., news and shopping websites). This latter class of sources are where our content preference findings can have immediate impact. Interestingly, we observed one outlier where participants reported high engagement with images: for SNS (specifically, with Facebook). Still, our findings illustrate that participants are seeking more from the image descriptions than is provided to them today and our findings offer insight in how to make such improvements.`,
        `Image In(Dependence): During our analysis of the data with respect to RQ2, "What are BLV peoples’ description preferences for digital images in these different sources?", we made the general observation that the source informs what one expects from a description of an image on that source. For some sources (e.g. dating websites), participants expressed interest in learning about the image as-is without taking additional information from the source (e.g. text) into consideration. For other sources, participants want the description to be based on additional information beyond just the image. For example, participants expected the information surrounding the image to drive what content would be described in an image for news sites, productivity documents, e-publications, and SNS. Accordingly, when developing processes to generate meaningful image descriptions it is important to be discerning about when and how to use the content surrounding the images.`,
        `Amount of Content: Also with respect to RQ2, we observed considerable diversity across sources in terms of participants’ desires for the amount of content and level of detail they want in a description. We offer a nuanced view of how participants’ content wants vary around source types in Table 2. For each source, we specify all the types of content from a lengthy list of options that at least one of our participants thought was important content to describe. We group these findings around three key themes that are commonly the central focuses of an image composition: event/scene, people, and objects/landmarks.`,
        `Notably, for some sources, the amount of content desired in an image description was greater than on other sources. For instance, we noted that participants want to have the most content available to them for images found on SNS, dating sites, and news websites, whereas there were fewer description content wants for images found on productivity applications and e-publications. We attribute this to the fact that our participants viewed images as a central focus of SNS, whereas images on productivity applications and e-publications were viewed as more decorative (which may not necessarily be an accurate assessment of the role of imagery in these sources). For other sources, the content focus was highly variable; e.g. for news websites, it was dependent on the news story.`,
        `Amount of Detail: As noted above, some images found on some sources may require more content than on others. That said, during our analysis we also noted that there are other factors that may impact the amount of content and/or the level of detail that is included in a description. For instance, we noted that the task one is involved in or the amount of time they have influence the amount of content they want. In P26’s words about news sources, "When I was younger I really loved it when people went all details...now I’m older I don’t really have time for that. It’s really nice when I know what’s going on, but I don’t have to know that a bird was flying over the people." We also noted that the level of detail one wants may be dependent on whether they previously had vision. In contrast to P26, we heard from P09 that in almost every circumstance that they wanted as many details as possible "because to me that helps paint the picture [...] I’d rather be on sensory overload." We also learned that for some, having all content available to them is an issue of equity/justice and/or personal interest, whereas others find too much information can be distracting, unhelpful, or boring.`,
      ],
      summary: {
        model1: {
          short: `FINDINGS 
`,
          medium: `FINDINGS 
`,
          long: `FINDINGS 
`,
        },
        model2: {
          short: `Participants shared that they read news articles but none of the image descriptions provided the information they needed in order to understand what was in the image. Participants said they want image descriptions that clarify the purpose of the images in news sources. For images where people are the central focus, the key content they want is the identifiable characteristics of the person or group and important details about their interactions or experiences.`,
          medium: `Participants shared that they read news articles, but none of the image descriptions provided the information they needed in order to understand what was in the image. Participants said they want image descriptions that clarify the purpose of the images in news sources. For images where people are the central focus, the key content they want centers on the identifiable characteristics of the person or group and important details about their interactions or experiences. Most participants reported a high engagement with descriptions in SNS descriptions in comparison to other descriptions in other places.`,
          long: `Participants shared that they read news articles, but none of the image descriptions provided the information they needed in order to understand what was in the image. Participants said they want image descriptions that clarify the purpose of the images in news sources. For images where people are the central focus, the key content they want centers on the identifiable characteristics of the person or group and important details about their interactions or experiences. Most participants reported a high engagement with descriptions in SNS descriptions in comparison to other descriptions in other places, such as Facebook and SNS. Most people use SNS (SNS) most of the descriptions in relation to descriptions in social networking sites (Facebook) Most participants said that they use descriptions in many places with respect to Facebook descriptions in these places.`,
        },
      },
    },
    {
      sectionHeading: `Discussion`,
      sectionContent: [
        `While it is already known that image descriptions are imperfect, our findings offer promising evidence that part of the reason may be because the one-size-fits-all approach that is widely-used today is inadequate. In what follows, we discuss how our findings relate to contemporary research followed by design recommendations for how to improve image captioning services and future research directions.`,
        `Comparison of Our Findings with Prior Work. Our findings provide new insight into BLV people’s description preferences for images found in and across seven source types. This work builds on Petrie et al.’s [30] claim that the description preferences of people who are BLV vary based on source, as well as existing image description guidelines, e.g. [13].`,
        `Our findings underscore types of description content that may be desired universally, across different sources. For instance, our participants consistently wanted to learn about people and objects across all sources (Table 2). This aligns with well-established guidelines [13, 11] and prior findings for images found on SNS [29, 39, 44]. Extending prior findings, our work also reveals that participants consistently wanted a description of text that is present in images across all sources.`,
        `Our work reinforces the importance of [31] by reporting that BLV people want dating platforms to be accessible. Further, in alignment with [31], we heard some of our participants express concern that a description of a person’s physical appearance can be very subjective. Our study enriches our understanding of this issue, highlighting how desired description properties can even be controversial. Take a person’s race as an example. Some participants noted that including information about a person’s race or ethnicity in an image description would be necessary when the image is paired with a story or post related to social justice or cultural interest. Other participants noted that it is important to have access to all of the same information a person who is sighted has–which would require this information to be disclosed. Still, others expressed concern about whether race or ethnicity can be accurately determined by a picture alone, where accuracy may only be determined by the person who is being represented. Our findings underscore the importance of connecting efforts on generating image descriptions to contemporary literature (e.g. on race and gender studies) to address how to handle some of the content areas that could be considered subjective or sensitive (e.g. race, gender, body shape, disability).`,
        `Design Recommendations for Next-Generation Image Description Services. We offer our findings about description content preferences of BLV people with respect to different sources as a valuable starting point to designing improved image description services for this population. This is relevant whether training professionals, training crowd workers, or creating large-scale datasets to train AI algorithms.`,
        `Our findings offer a tangible guide regarding what information is preferred for seven sources. Developers could use the taxonomy in Table 2 to support source-specific description guidelines or templates for human-authored descriptions. Already, Morash et al. [26] have found STEM-specific templates improve alt text in textbooks; creating templates for other domains may therefore be useful. Alternatively, our taxonomy could be used to redesign instructions given to crowd workers, when authoring image descriptions that are used for training AI models, and to support inclusion of relevant details depending on image context.`,
        `In addition, our findings reveal that some description wants are more general, meaning they can be applied to all sources, whereas others only apply to a few or one source. For instance, all image descriptions should include text and identification of people and objects, whereas information about tattoos, lighting, hair style, and damage were only wanted on one source (and these sources varied). This knowledge could be useful in prioritizing the relative importance of gathering data or training models to include certain categories of information.`,
        `Our findings also reveal that the description wants our participants specified often go well beyond capabilities of current vision-to-language AI systems. For instance, we found that multimodal analysis of all of the media on the source surrounding the image (e.g. text, video) is necessary, in some cases, to devise a meaningful image description. In particular, appropriate descriptions for images found on news sites, SNS, productivity applications, and e-publications greatly depend on the surrounding content. Yet, today’s AI systems only observe the image when generating a description. Future algorithms should be able to identify when and what surrounding content is needed to create a description.`,
        `Notably, many people who are BLV do not trust the descriptions provided by today’s AI systems [25]. Inclusion of this population at the early stages of the innovation of technologies is one step towards ensuring trustworthiness of image descriptions. This aligns with contemporary discussion that emphasizes the need for "protecting people who fall outside of the ‘norms’ reflected and constructed by AI systems" [21], ensuring fairness [18] or justice [22] in AI for people with disabilities, and aligning with other ethical considerations [27].`,
        `Future Work. Despite our guide for what content is preferred when, we still believe ongoing, larger-scale analysis is important. We note a few ways in which future work could extend our study. One valuable direction is to examine participants’ diversity in perspectives based on how much exposure they had to visual information, whether that is based on the level and time of onset of their vision loss, their training in visual literacy, and their direct experience with the objects or phenomena represented in an image. A further factor that may bear influence pertains to the use-case in which a person intends to use the image. Additionally, it’s clear that some people prefer more detail while others prefer less. For some, having all content available to them is an issue of equity/justice and/or personal interest, whereas others find too much information can be distracting, unhelpful, or boring. In addition, when a person does not have prior experience with the content area or a similar cultural reference point, a higher degree of detail (and/or additional modes of representation) may be needed for a person to create a mental image or approximate reference, as noted in [12, 26]. Valuable future research includes personalizing descriptions so that in addition to consideration of source, there also is consideration of each person’s preference for the level and type of description for each source.`,
        `Relating to our above study suggestions, we believe that next-generation image description systems might also benefit from including features that: 1) enable the user to specify the quantity of content described; 2) enable users to decide what level or precision of language that they want in a description (e.g. dark blue, vs. space blue 1C2951–RGB 294181, HEX 294181) or domain-specific language (e.g. the architectural style of buildings on a college campus). Based on this research we also hypothesize that the following features might assist people who are BLV to locate images and engage in determining the right description for them. These ideas include: 1) providing the option to read a series of descriptions written for the same image to empower an individual to learn about different description styles and assess the accuracy of a description to the surrounding context; and 2) presenting image descriptions before or after the main body of text.`,
        `We also heard from participants that they would like to be able to ask for descriptions on demand (as opposed to depending solely on alt text or existing descriptions); such an opportunity would address a series of underlying concerns about descriptions, including the inequity faced by not getting the same information as others and discomfort about receiving incomplete descriptions. These findings affirm the need for further research related to next-generation, interactive technologies for describing images [3, 19, 28].`,
      ],
      summary: {
        model1: {
          short: `While it is already known that image descriptions are imperfect, our findings offer promising evidence that part of the reason may be because the one-size-fits-all approach that is widely-used today is inadequate. Comparison of Our Findings with Prior Work. Our findings provide new insight into BLV people’s description preferences for images found in and across seven source types. ’s [30] claim that the description preferences of people who are BLV vary based on source, as well as existing image description guidelines, e.g. [13]. Design Recommendations for Next-Generation Image Description Services. Yet, today’s AI systems only observe the image when generating a description. Despite our guide for what content is preferred when, we still believe ongoing, larger-scale analysis is important. We note a few ways in which future work could extend our study. Relating to our above study suggestions, we believe that next-generation image description systems might also benefit from including features that: 1) enable the user to specify the quantity of content described; 2) enable users to decide what level or precision of language that they want in a description (e.g. dark blue, vs. space blue 1C2951–RGB 294181, HEX 294181) or domain-specific language (e.g. the architectural style of buildings on a college campus). These findings affirm the need for further research related to next-generation, interactive technologies for describing images [3, 19, 28].`,
          medium: `While it is already known that image descriptions are imperfect, our findings offer promising evidence that part of the reason may be because the one-size-fits-all approach that is widely-used today is inadequate. In what follows, we discuss how our findings relate to contemporary research followed by design recommendations for how to improve image captioning services and future research directions. Comparison of Our Findings with Prior Work. Our findings provide new insight into BLV people’s description preferences for images found in and across seven source types. This aligns with well-established guidelines [13, 11] and prior findings for images found on SNS [29, 39, 44]. Our work reinforces the importance of [31] by reporting that BLV people want dating platforms to be accessible. Further, in alignment with [31], we heard some of our participants express concern that a description of a person’s physical appearance can be very subjective. Some participants noted that including information about a person’s race or ethnicity in an image description would be necessary when the image is paired with a story or post related to social justice or cultural interest. Our findings underscore the importance of connecting efforts on generating image descriptions to contemporary literature (e.g. on race and gender studies) to address how to handle some of the content areas that could be considered subjective or sensitive (e.g. race, gender, body shape, disability). Design Recommendations for Next-Generation Image Description Services. Our findings offer a tangible guide regarding what information is preferred for seven sources. Alternatively, our taxonomy could be used to redesign instructions given to crowd workers, when authoring image descriptions that are used for training AI models, and to support inclusion of relevant details depending on image context. For instance, all image descriptions should include text and identification of people and objects, whereas information about tattoos, lighting, hair style, and damage were only wanted on one source (and these sources varied). Yet, today’s AI systems only observe the image when generating a description. Inclusion of this population at the early stages of the innovation of technologies is one step towards ensuring trustworthiness of image descriptions. Despite our guide for what content is preferred when, we still believe ongoing, larger-scale analysis is important. We note a few ways in which future work could extend our study. Additionally, it’s clear that some people prefer more detail while others prefer less. Relating to our above study suggestions, we believe that next-generation image description systems might also benefit from including features that: 1) enable the user to specify the quantity of content described; 2) enable users to decide what level or precision of language that they want in a description (e.g. dark blue, vs. space blue 1C2951–RGB 294181, HEX 294181) or domain-specific language (e.g. the architectural style of buildings on a college campus).`,
          long: `While it is already known that image descriptions are imperfect, our findings offer promising evidence that part of the reason may be because the one-size-fits-all approach that is widely-used today is inadequate. In what follows, we discuss how our findings relate to contemporary research followed by design recommendations for how to improve image captioning services and future research directions. Comparison of Our Findings with Prior Work. Our findings provide new insight into BLV people’s description preferences for images found in and across seven source types. ’s [30] claim that the description preferences of people who are BLV vary based on source, as well as existing image description guidelines, e.g. [13]. For instance, our participants consistently wanted to learn about people and objects across all sources (Table 2). Extending prior findings, our work also reveals that participants consistently wanted a description of text that is present in images across all sources. Our work reinforces the importance of [31] by reporting that BLV people want dating platforms to be accessible. Our study enriches our understanding of this issue, highlighting how desired description properties can even be controversial. Design Recommendations for Next-Generation Image Description Services. This is relevant whether training professionals, training crowd workers, or creating large-scale datasets to train AI algorithms. Our findings offer a tangible guide regarding what information is preferred for seven sources. Developers could use the taxonomy in Table 2 to support source-specific description guidelines or templates for human-authored descriptions. 26] have found STEM-specific templates improve alt text in textbooks; creating templates for other domains may therefore be useful. For instance, all image descriptions should include text and identification of people and objects, whereas information about tattoos, lighting, hair style, and damage were only wanted on one source (and these sources varied). This knowledge could be useful in prioritizing the relative importance of gathering data or training models to include certain categories of information. Yet, today’s AI systems only observe the image when generating a description. Future algorithms should be able to identify when and what surrounding content is needed to create a description. Inclusion of this population at the early stages of the innovation of technologies is one step towards ensuring trustworthiness of image descriptions. Despite our guide for what content is preferred when, we still believe ongoing, larger-scale analysis is important. We note a few ways in which future work could extend our study. A further factor that may bear influence pertains to the use-case in which a person intends to use the image. Additionally, it’s clear that some people prefer more detail while others prefer less. In addition, when a person does not have prior experience with the content area or a similar cultural reference point, a higher degree of detail (and/or additional modes of representation) may be needed for a person to create a mental image or approximate reference, as noted in [12, 26]. Relating to our above study suggestions, we believe that next-generation image description systems might also benefit from including features that: 1) enable the user to specify the quantity of content described; 2) enable users to decide what level or precision of language that they want in a description (e.g. dark blue, vs. space blue 1C2951–RGB 294181, HEX 294181) or domain-specific language (e.g. the architectural style of buildings on a college campus). Based on this research we also hypothesize that the following features might assist people who are BLV to locate images and engage in determining the right description for them. We also heard from participants that they would like to be able to ask for descriptions on demand (as opposed to depending solely on alt text or existing descriptions); such an opportunity would address a series of underlying concerns about descriptions, including the inequity faced by not getting the same information as others and discomfort about receiving incomplete descriptions. These findings affirm the need for further research related to next-generation, interactive technologies for describing images [3, 19, 28].`,
        },
        model2: {
          short: `Findings provide new insight into BLV people’s description preferences for images found in and across seven source types. Participants consistently wanted to learn about people and objects across all sources. Findings underscore types of description content that may be desired universally, across different sources.`,
          medium: `Findings provide new insight into BLV people’s description preferences for images found in and across seven source types. Participants consistently wanted to learn about people and objects across all sources. Findings underscore types of description content that may be desired universally, across different sources. This knowledge could be useful in prioritizing the relative importance of gathering data or training models to include certain categories of information. Our taxonomy could be used to redesign instructions given to crowd workers, when authoring image descriptions that are used for training AI models, and to support inclusion of relevant details.`,
          long: `Findings provide new insight into BLV people’s description preferences for images found in and across seven source types. Participants consistently wanted to learn about people and objects across all sources. Findings underscore types of description content that may be desired universally, across different sources. This knowledge could be useful in prioritizing the relative importance of gathering data or training models to include certain categories of information. Our taxonomy could be used to redesign instructions given to crowd workers, when authoring image descriptions that are used for training AI models, and to support inclusion of relevant details depending on image context. Future AI systems should be able to identify and create a description when generating a description. Notably, many people who do not trust descriptions provided today are not the descriptions provided`,
        },
      },
    },
    {
      sectionHeading: `Conclusion`,
      sectionContent: [
        `In this study, we took a holistic approach to examining BLV people’s experiences with digital images found on different sources, and the variance of their description preferences across sources. The findings we present in this paper may be used by scholars and practitioners who are working to refine the ways in which image descriptions are generated by human-powered services, AI-powered services, and hybrid services for generating image descriptions. Ensuring image accessibility for people who are BLV is particularly important given the widespread proliferation of visual media. Such descriptions may also benefit sighted users, such as when accessing media eyes-free (i.e., via a voice agent such as Alexa or Cortana), and by providing additional metadata that can support information retrieval. Developing and evaluating source-dependent image descriptions based on the guidelines presented herein is a promising area for future study.`,
      ],
      summary: {
        model1: {
          short: `In this study, we took a holistic approach to examining BLV people’s experiences with digital images found on different sources, and the variance of their description preferences across sources. The findings we present in this paper may be used by scholars and practitioners who are working to refine the ways in which image descriptions are generated by human-powered services, AI-powered services, and hybrid services for generating image descriptions.`,
          medium: `In this study, we took a holistic approach to examining BLV people’s experiences with digital images found on different sources, and the variance of their description preferences across sources. Developing and evaluating source-dependent image descriptions based on the guidelines presented herein is a promising area for future study.`,
          long: `In this study, we took a holistic approach to examining BLV people’s experiences with digital images found on different sources, and the variance of their description preferences across sources. The findings we present in this paper may be used by scholars and practitioners who are working to refine the ways in which image descriptions are generated by human-powered services, AI-powered services, and hybrid services for generating image descriptions. Ensuring image accessibility for people who are BLV is particularly important given the widespread proliferation of visual media. Developing and evaluating source-dependent image descriptions based on the guidelines presented herein is a promising area for future study.`,
        },
        model2: {
          short: `The findings may be used by scholars and practitioners who are working to refine the ways in which image descriptions are generated by human-powered services. Ensuring image accessibility for people who are BLV is particularly important given the widespread proliferation of visual media. Such descriptions may also benefit sighted users, such as when accessing media eyes-free.`,
          medium: `The findings may be used by scholars and practitioners who are working to refine the ways in which image descriptions are generated by human-powered services. Ensuring image accessibility for people who are BLV is particularly important given the widespread proliferation of visual media. Such descriptions may also benefit sighted users, such as when accessing media eyes-free (i.e., via a voice agent such as Alexa or Cortana), and by providing additional metadata that can support information retrieval. Developing and evaluating source-dependent image descriptions based on the guidelines presented herein is a promising area for future study.`,
          long: `The findings may be used by scholars and practitioners who are working to refine the ways in which image descriptions are generated by human-powered services. Ensuring image accessibility for people who are BLV is particularly important given the widespread proliferation of visual media. Such descriptions may also benefit sighted users, such as when accessing media eyes-free (i.e., via a voice agent such as Alexa or Cortana), and by providing additional metadata that can support information retrieval. Developing and evaluating source-dependent image descriptions based on the guidelines presented herein is a promising area for future study. Developing such descriptions is a potential area of future study, says the authors of this paper. Back to Mail Online home: Back to the page you came from.`,
        },
      },
    },
  ],
};

const sectionsData = {
  "abstract-introduction": `When prototyping AI experiences (AIX), interface designers seek useful and usable ways to support end-user tasks through AI capabilities. We propose Model-Informed Prototyping (MIP), a workflow for AIX design that combines model exploration with UI prototyping tasks.`,
  "abstract-related-work": `When prototyping AI experiences (AIX), interface designers seek useful and usable ways to support end-user tasks through AI capabilities. By assessing those alternatives against user needs (e.g., fast to unlock, secure, low cognitive effort to remember), the designer will finalize the UI design. AI-powered applications bring additional challenges to UI prototyping. AI features introduce dynamic behavior due to the scope of training data, system use over time, and variations in input data individual users contribute and the potential to learn from outcomes. Thus, designers must identify the interactions between user task-flows and AI capabilities [14, 16, 35] in order to design the user interface for AI experiences (AIX). By assuming a ‘black-box’ view of AI, tools make it challenging for designers to access necessary AI attributes during the design process [62]. Thus, the motivating question for our work is: How might prototyping tools allow designers to directly incorporate target AI features during rapid and iterative prototyping? To accomplish this, we propose Model-Informed Prototyping (MIP), a workflow that combines model exploration and interface design tasks. In the data previews tab, ProtoAI automatically generates previews of the interface for each input data, allowing the designer to evaluate the designed AIX for diverse inputs (Figure 1c). MIP streamlines model exploration and UX design tasks during the prototyping process for AI-powered interfaces.`,
  "abstract-design-consideration": `When prototyping AI experiences (AIX), interface designers seek useful and usable ways to support end-user tasks through AI capabilities. However, AI poses challenges to design due to its dynamic behavior in response to training data, end-user data, and feedback. Designers must consider AI’s uncertainties and offer adaptations such as explainability, error recovery, and automation vs. human task control. AI-powered applications bring additional challenges to UI prototyping. Thus, designers must identify the interactions between user task-flows and AI capabilities [14, 16, 35] in order to design the user interface for AI experiences (AIX). Unfortunately, current UI prototyping tools lack support for designing AI-powered interfaces [68]. Thus, the motivating question for our work is: How might prototyping tools allow designers to directly incorporate target AI features during rapid and iterative prototyping? They also need to identify different kinds of interface breakdowns such as mismatch with end-user expectations, low utility (high cost) from using AI, and data specific failures and offer repairs to recover the user experience. In the data previews tab, ProtoAI automatically generates previews of the interface for each input data, allowing the designer to evaluate the designed AIX for diverse inputs (Figure 1c). By extending the familiar design paradigm of current prototyping tools, ProtoAI allows designers to operationalize HAI design guidelines within their created designs. Standard UI prototyping tools such as Wireframe. cc [66], Figma [20], and Adobe XD [1] allow designers to work at the user interface level alone through horizontal prototyping [5]. This requires a form of vertical prototyping in which designers can access specifications about the underlying AI implementation and map them to AI-powered interfaces [5, 64]. A recommended workflow for UI prototyping consists of three phases: design, test, and analysis. Further, given most designers’ limited expertise with AI [70], prototyping tools should make AI features more accessible, immediate (support rapid iterative feedback, reflection-in-action, and reflection-on-action), and generative (allow test, probe, and exploration iterations) for UI designers [13, 27, 41]. In ProtoAI we automatically generate interface alternatives by invoking built-in models with input data provided by designers. Inspired by this approach, in ProtoAI, we allow designers to configure desired behavior (ground truth) by providing model output data for comparison (i.e., designer as wizard [8]). For instance, through iterative UI experimentation, Quick Access identified UI needs to offer proactive recommendations [60].`,
  "abstract-model-informed-prototyping": `When prototyping AI experiences (AIX), interface designers seek useful and usable ways to support end-user tasks through AI capabilities. Designers must consider AI’s uncertainties and offer adaptations such as explainability, error recovery, and automation vs. human task control. This introduces friction to rapid and iterative prototyping. AI-powered applications bring additional challenges to UI prototyping. Thus, designers must identify the interactions between user task-flows and AI capabilities [14, 16, 35] in order to design the user interface for AI experiences (AIX). By exploring AI’s capabilities and limitations through prototyping, they need to design interface adaptations such as explanations for AI outputs, seamless handling of AI failures, and collecting user feedback to improve the AI [4]. To prototype AI features, designers currently need to work with multiple tools to explore AI behavior (e.g., [51]), probe its capabilities and limitations [24], and evaluate their design with diverse user inputs (e.g., skin-tone, lighting conditions, camera angle, facial features such as beard, glasses, or a mask) [11]. They also need to identify different kinds of interface breakdowns such as mismatch with end-user expectations, low utility (high cost) from using AI, and data specific failures and offer repairs to recover the user experience. To accomplish this, we propose Model-Informed Prototyping (MIP), a workflow that combines model exploration and interface design tasks. Standard UI prototyping tools such as Wireframe. This requires a form of vertical prototyping in which designers can access specifications about the underlying AI implementation and map them to AI-powered interfaces [5, 64]. A recommended workflow for UI prototyping consists of three phases: design, test, and analysis. 2.2 Test 
AIX designers need to map AI-to-interface features, identify gulfs of execution and evaluation, and assess visual aesthetics for AI features. In Topiary [42], designers create a map that models people’s location, which the Wizard uses to update locations during testing. For instance, D.tools offers a ‘group analysis’ mode aggregating data from multiple user sessions into one view [29]. In this regard, both academic and industry sources have put forth design guidelines about good AIX design [4, 23, 37]. We collected a total of 284 Human-AI design guidelines. We conducted inductive in-vivo coding to synthesize the main objectives and tasks for designers and the corresponding AI components necessary to accomplish those tasks. D1: Prototyping tools should allow designers to invoke ML models by specifying input data directly. This will allow designers to incorporate AI features into end-user tasks appropriately. D2: Prototyping tools should allow designers to incorporate AI outputs into interface design. Based on decisions about AI feature integration into interface design, designers may need to revisit the model inputs and outputs (i.e., the API). With conventional applications, design typically converges to a set of standard features across all users. With this intent, HAI guidelines recommend applications should be designed to work across a diverse set of users, use cases, and contexts of use. For example, “while all errors are equal to an ML system, not all errors are equal to all people. [ 23]” To operationalize these recommendations, designers need to evaluate their interface choices across diverse data. Third, a data context can be bound to an interface state.`,
  "abstract-design-scenarios": `When prototyping AI experiences (AIX), interface designers seek useful and usable ways to support end-user tasks through AI capabilities. However, AI poses challenges to design due to its dynamic behavior in response to training data, end-user data, and feedback. This introduces friction to rapid and iterative prototyping. When prototyping potential designs for user interfaces (UI), designers work to transform end-user needs into interface specifications [67]. By assessing those alternatives against user needs (e.g., fast to unlock, secure, low cognitive effort to remember), the designer will finalize the UI design. Thus, the motivating question for our work is: How might prototyping tools allow designers to directly incorporate target AI features during rapid and iterative prototyping? Further, ProtoAI automatically generates data previews of the UI for differing input data, allowing designers to evaluate designs for breakdowns across diverse scenarios and contexts of use. In the data previews tab, ProtoAI automatically generates previews of the interface for each input data, allowing the designer to evaluate the designed AIX for diverse inputs (Figure 1c). MIP streamlines model exploration and UX design tasks during the prototyping process for AI-powered interfaces. Standard UI prototyping tools such as Wireframe. cc [66], Figma [20], and Adobe XD [1] allow designers to work at the user interface level alone through horizontal prototyping [5]. However, when designing AI-powered applications, both the end-user task requirements and the underlying AI components needs to be mapped onto the user interface syntax [12, 14, 23]. A recommended workflow for UI prototyping consists of three phases: design, test, and analysis. In ProtoAI we take a similar approach and allow designers to incorporate input data and ML model outputs into UI prototypes (e.g., designing password meters by mapping scores from neural networks and heuristics to a visual bar [61]). Designers need similar analysis and visualization tools at the interface level that will allow them to identify mismatches in model behavior. During iterative prototyping, the goal is to identify breakdowns in design and offer fixes [7, 22, 26, 54, 65]. In this regard, both academic and industry sources have put forth design guidelines about good AIX design [4, 23, 37]. More importantly, the guidelines prescribe design ‘fixes’ to lower end-user impact from AI-breakdowns such as (1) end-user context breakdown: AI performs poorly for some user-data and in some usage contexts; (2) expectation breakdown: AI behavior and outputs do not align with end-user mental models; and (3) task-utility breakdown: higher cost of using AI due to its failure to understand end-user goals. To address these breakdowns, designers need access to the underlying AI model, features, and output data for diverse end-user inputs. D1: Prototyping tools should allow designers to invoke ML models by specifying input data directly. D2: Prototyping tools should allow designers to incorporate AI outputs into interface design. When presenting statistical or numeric outputs, the designer needs to consider factors such as precision and rounding. With conventional applications, design typically converges to a set of standard features across all users. However, with AI models, we can personalize the end-user experience to highly specific contexts. For example, “while all errors are equal to an ML system, not all errors are equal to all people. [ Third, a data context can be bound to an interface state. Designers can evaluate the design for diverse users and contexts by generating interface previews for different data contexts. Designers should be able to switch between model simulation, design, test, analysis, and revision and repair. Based on design considerations, we implemented ProtoAI to prototype AI-powered interfaces for AIX design. After creating the Boolean column, Divya returns to the UI tab to address the false-negative instances (D5). Divya adds a new interface state to the unlock screen conditioned on the Boolean column value, which she configures using the properties panel (Figure 4a). When she returns to the previews view, Divya sees that instances of false negatives have the prompt message she just created. Through this metadata, we generate the client-side model cards. The client is implemented using HTML and JavaScript. Divya’s company has already assigned an engineering team to the project, and they have been working on an initial version of the Face-ID model. Divya selects the company’s Face-ID model and navigates to the Data tab. The Data tab will allow her to import input data for different personas and scenarios of use. Column types are made distinct through colorcoding. The sidebar view shows a model card [50] for each model selected. From the Face-ID model card, Divya sees that the model requires images (both for training/registration) and optional ground truth labels. From the faceted table, Divya selects the cell value with the persona’s face image and clicks on the ‘Add to Wireframe’ button. Based on AIX interface design patterns, ProtoAI implements an initial set of widgets for binding Boolean values to images, categorizing items by tags, and showing ranked order of items. The widget library can be extended in the future to support additional layout design needs. In that case, Divya can configure the data with different cluster sizes and compare the results in the data previews view. By checking the ‘show explanations’ flag in the sidebar, Divya can see the match score element she added in the UI tab.`,
  "abstract-preliminary-evaluation": `When prototyping AI experiences (AIX), interface designers seek useful and usable ways to support end-user tasks through AI capabilities. Designers must consider AI’s uncertainties and offer adaptations such as explainability, error recovery, and automation vs. human task control. Thus, designers must identify the interactions between user task-flows and AI capabilities [14, 16, 35] in order to design the user interface for AI experiences (AIX). In the process, AIX designers also need to assess interface choices against diverse users and contexts of use. To prototype AI features, designers currently need to work with multiple tools to explore AI behavior (e.g., [51]), probe its capabilities and limitations [24], and evaluate their design with diverse user inputs (e.g., skin-tone, lighting conditions, camera angle, facial features such as beard, glasses, or a mask) [11]. Thus, the motivating question for our work is: How might prototyping tools allow designers to directly incorporate target AI features during rapid and iterative prototyping? Further, ProtoAI automatically generates data previews of the UI for differing input data, allowing designers to evaluate designs for breakdowns across diverse scenarios and contexts of use. After running the model, the designer can prototype the Face-ID user interface using one of the input faces and corresponding model outputs (Figure 1b). MIP streamlines model exploration and UX design tasks during the prototyping process for AI-powered interfaces. Standard UI prototyping tools such as Wireframe. This requires a form of vertical prototyping in which designers can access specifications about the underlying AI implementation and map them to AI-powered interfaces [5, 64]. A recommended workflow for UI prototyping consists of three phases: design, test, and analysis. Further, given most designers’ limited expertise with AI [70], prototyping tools should make AI features more accessible, immediate (support rapid iterative feedback, reflection-in-action, and reflection-on-action), and generative (allow test, probe, and exploration iterations) for UI designers [13, 27, 41]. Building on existing UX practice, designers may consider approaches such as constructing personas with varying quantitative data [53]. For instance, Suede implements electronically supportedWoZ testing techniques that generate chat messages using test data [38]. Inspired by this approach, in ProtoAI, we allow designers to configure desired behavior (ground truth) by providing model output data for comparison (i.e., designer as wizard [8]). In this regard, both academic and industry sources have put forth design guidelines about good AIX design [4, 23, 37]. We collected a total of 284 Human-AI design guidelines. To address these breakdowns, designers need access to the underlying AI model, features, and output data for diverse end-user inputs. This will allow designers to incorporate AI features into end-user tasks appropriately. For instance, designers may need to split complex outputs and explanations into multiple parts and present them one at a time. With conventional applications, design typically converges to a set of standard features across all users. D5: Prototyping tools should allow flexibility for designers to incorporate model-related data rapidly and iteratively. Third, a data context can be bound to an interface state. When prototyping for AI features, tools should allow designers to navigate across this design space flexibly. In this state, Divya adds a message at the top of the screen prompting the user to move closer to the screen. When she returns to the previews view, Divya sees that instances of false negatives have the prompt message she just created. We use the metadata format in RunwayML [51] to specify the inputs and outputs to the model. Through this metadata, we generate the client-side model cards. The client is implemented using HTML and JavaScript. Column types are made distinct through colorcoding. The sidebar view shows a model card [50] for each model selected. From the Face-ID model card, Divya sees that the model requires images (both for training/registration) and optional ground truth labels. In an alternate scenario, in the absence of a pre-existing model, Divya can use the spreadsheet view to “draft” desired model behavior and outputs for different input data and share those specifications with her engineering team. Divya first adds the emergency button by selecting the button element from the UI elements tab. For example, in a different scenario, Divya can set the data context to all images a persona has taken (e.g., for a photo album interface). For other complex layout needs, Divya can select entire columns, or brush select the desired data from the data table and add them to the interface as a widget. 4.3 Design Evaluation 
At this point, Divya has an initial wireframe of the phone unlock interface designed using the portrait image from a single persona. She selects the Data Previews tab to evaluate her current design against different personas and their photos. ProtoAI automatically instantiates the screen interface based on the data context and using all data imported in the data tab (D4). The preview view allows Divya to rapidly evaluate her design as it is being created and conduct design checks. This will allow Divya to visually see how the AI-powered interface responds after differing degrees of use. In the sidebar, ProtoAI provides a summary of each tag indicating the number of instances with that tag. By checking the ‘show explanations’ flag in the sidebar, Divya can see the match score element she added in the UI tab. In this example, Divya sees that the model fails when the person is farther away or when their eyes are closed. Back in the preview mode, the designer sees that cropping fails when there are multiple salient points and no salient points. To suggest a fix, the designer proposes an image widget for users that pans between different salient points in a loop. ProtoAI can help simulate design previews over time and use. For example, we imagine a designer using ProtoAI to design a movie recommendation page. By looking at the confidence score for recommendations, the designer creates a new screen state for low confidence recommendations: instead of showing the movies, it asks end-users to select movie genres of interest. Further by looking at the explanation factors, the designer can incorporate model explanations in text form: “Because you watched [explanation value], we think you might like:” Through data personas with differing inputs, ProtoAI allows designers to simulate model behavior over time of use, and design appropriate interface experiences. 5.3 Chat Assistant–Mixed Initiative Design 
A guiding principle for integrating AI capabilities into task workflows is to determine the utility of the AI for end users [36].`,
  "abstract-discussion-and-future-work": `When prototyping AI experiences (AIX), interface designers seek useful and usable ways to support end-user tasks through AI capabilities. For instance, to design a phone ‘unlock’ user experience (UX), the designer may consider interface alternatives—such as an alphanumeric password, a numeric passcode, or pattern-based unlocking— to allow end-users to input identifying information for access. AI-powered applications bring additional challenges to UI prototyping. Thus, designers must identify the interactions between user task-flows and AI capabilities [14, 16, 35] in order to design the user interface for AI experiences (AIX). Through an analysis of current human-AI (HAI) design guidelines from academic and industry sources [4, 23, 37], we identified a set of needs and design considerations for AI prototyping tools. As shown in Figure 1a, to design a Face-ID phone unlock AIX, the designer can begin with a diverse set of registered and new faces along with ground truth data as inputs to the Face Identification model. In the data previews tab, ProtoAI automatically generates previews of the interface for each input data, allowing the designer to evaluate the designed AIX for diverse inputs (Figure 1c). ProtoAI automatically tags errors such as false positives and false negatives based on ground truth data. MIP streamlines model exploration and UX design tasks during the prototyping process for AI-powered interfaces. Standard UI prototyping tools such as Wireframe. This requires a form of vertical prototyping in which designers can access specifications about the underlying AI implementation and map them to AI-powered interfaces [5, 64]. A recommended workflow for UI prototyping consists of three phases: design, test, and analysis. Further, given most designers’ limited expertise with AI [70], prototyping tools should make AI features more accessible, immediate (support rapid iterative feedback, reflection-in-action, and reflection-on-action), and generative (allow test, probe, and exploration iterations) for UI designers [13, 27, 41]. In ProtoAI we automatically generate interface alternatives by invoking built-in models with input data provided by designers. This lets designers experience the UI’s design first hand [9]. For instance, D.tools offers a ‘group analysis’ mode aggregating data from multiple user sessions into one view [29]. We collected a total of 284 Human-AI design guidelines. Based on decisions about AI feature integration into interface design, designers may need to revisit the model inputs and outputs (i.e., the API). For instance, designers may need to split complex outputs and explanations into multiple parts and present them one at a time. When presenting statistical or numeric outputs, the designer needs to consider factors such as precision and rounding. D4: Prototyping tools should allow designers to evaluate design choices across diverse users and usage contexts. With conventional applications, design typically converges to a set of standard features across all users. However, with AI models, we can personalize the end-user experience to highly specific contexts. For example, “while all errors are equal to an ML system, not all errors are equal to all people. [ 23]” To operationalize these recommendations, designers need to evaluate their interface choices across diverse data. D5: Prototyping tools should allow flexibility for designers to incorporate model-related data rapidly and iteratively. Third, a data context can be bound to an interface state. Designers should be able to switch between model simulation, design, test, analysis, and revision and repair. After creating the Boolean column, Divya returns to the UI tab to address the false-negative instances (D5). In this state, Divya adds a message at the top of the screen prompting the user to move closer to the screen. This allows Divya to see the probabilistic nature of the AI’s task-flows to ensure that all users meet the desired goals. The server was written in Python and hosts different ML models. We use the metadata format in RunwayML [51] to specify the inputs and outputs to the model. Through this metadata, we generate the client-side model cards. This allows a number of already available models to be used in ProtoAI. The Data tab will allow her to import input data for different personas and scenarios of use. The sidebar view shows a model card [50] for each model selected. ProtoAI extends the spreadsheet and appends additional columns with model outputs. Divya first adds the emergency button by selecting the button element from the UI elements tab. For example, in a different scenario, Divya can set the data context to all images a persona has taken (e.g., for a photo album interface). From the faceted table, Divya selects the cell value with the persona’s face image and clicks on the ‘Add to Wireframe’ button. For other complex layout needs, Divya can select entire columns, or brush select the desired data from the data table and add them to the interface as a widget. The widget library can be extended in the future to support additional layout design needs. She selects the Data Previews tab to evaluate her current design against different personas and their photos. In a different scenario, to evaluate model functionality over time with model learning, Divya can configure data for different personas by providing different amounts of input data. Divya can also add her own tags to indicate domain-specific types of breakdowns or repairs. The Data tab allows for several different types of data transformations, including the categorization of numerical values (e.g., high, medium, and low), mapping transformations of model-assigned labels and values to end-user-friendly labels, calculating the minimum and maximum values, and custom formula functions for excel-like computation by specifying column headers and cells values to include in the computation (see Figure 3d). By tagging the images with appropriate labels, the designer sees that around 30% of images in the dataset have multiple salient points, and 5% have no salient points. ProtoAI can help simulate design previews over time and use. For example, we imagine a designer using ProtoAI to design a movie recommendation page. Further by looking at the explanation factors, the designer can incorporate model explanations in text form: “Because you watched [explanation value], we think you might like:” Through data personas with differing inputs, ProtoAI allows designers to simulate model behavior over time of use, and design appropriate interface experiences. Six participants had prior experience designing AI-powered applications. 6.1 Findings 
6.1.1 Model-Informed Prototyping with ProtoAI. Across all sessions, designers created an image recommendation UX with one or more screens (Figure 5). They used data previews as they worked and created new screen states based on the generated previews. In five of the sessions, designers directly started the activity using data elements, including persona images and quality scores. For instance, in session 1, the designer created a new task-flow for end-users to crop the image based on salient regions indicated by the heatmap and re-compute the quality score. In session 5, the designer suggested addressing tasks with no high-quality images by showing the CAM view to end-users and allowing them to retake the photo. They all mentioned their current datadriven design workflow either by writing code or analyzing data using spreadsheets. This tool makes it easy for me to carry out the entire flow on my own.” When commenting about prototyping for data instances, P5 commented: “The hardest thing about designing for AI is getting the right data. You can make something look good with fake labels and ‘ipsum-lorem,’ but using real data to mock things up helps you see where things are broken. Participants rated their learnability (i.e., can learn it quickly) a mean score of 5.33 (SD = 1.65), and learning without written instructions as 3.22 (SD = 1.39).`,
  "abstract-conclusion": `When prototyping AI experiences (AIX), interface designers seek useful and usable ways to support end-user tasks through AI capabilities. This introduces friction to rapid and iterative prototyping. We propose Model-Informed Prototyping (MIP), a workflow for AIX design that combines model exploration with UI prototyping tasks. In the process, AIX designers also need to assess interface choices against diverse users and contexts of use. To prototype AI features, designers currently need to work with multiple tools to explore AI behavior (e.g., [51]), probe its capabilities and limitations [24], and evaluate their design with diverse user inputs (e.g., skin-tone, lighting conditions, camera angle, facial features such as beard, glasses, or a mask) [11]. Thus, the motivating question for our work is: How might prototyping tools allow designers to directly incorporate target AI features during rapid and iterative prototyping? As shown in Figure 1a, to design a Face-ID phone unlock AIX, the designer can begin with a diverse set of registered and new faces along with ground truth data as inputs to the Face Identification model. In the data previews tab, ProtoAI automatically generates previews of the interface for each input data, allowing the designer to evaluate the designed AIX for diverse inputs (Figure 1c). ProtoAI automatically tags errors such as false positives and false negatives based on ground truth data. MIP streamlines model exploration and UX design tasks during the prototyping process for AI-powered interfaces. Standard UI prototyping tools such as Wireframe. This requires a form of vertical prototyping in which designers can access specifications about the underlying AI implementation and map them to AI-powered interfaces [5, 64]. A recommended workflow for UI prototyping consists of three phases: design, test, and analysis. Building on existing UX practice, designers may consider approaches such as constructing personas with varying quantitative data [53]. The What-if tool [24] allows designers to see the confusion matrix for binary classifiers visually [24]. In this regard, both academic and industry sources have put forth design guidelines about good AIX design [4, 23, 37]. We collected a total of 284 Human-AI design guidelines. D1: Prototyping tools should allow designers to invoke ML models by specifying input data directly. D2: Prototyping tools should allow designers to incorporate AI outputs into interface design. Based on decisions about AI feature integration into interface design, designers may need to revisit the model inputs and outputs (i.e., the API). For instance, designers may need to split complex outputs and explanations into multiple parts and present them one at a time. However, with AI models, we can personalize the end-user experience to highly specific contexts. For example, “while all errors are equal to an ML system, not all errors are equal to all people. [ Third, a data context can be bound to an interface state. Designers can evaluate the design for diverse users and contexts by generating interface previews for different data contexts. When prototyping for AI features, tools should allow designers to navigate across this design space flexibly. Designers should be able to switch between model simulation, design, test, analysis, and revision and repair. After creating the Boolean column, Divya returns to the UI tab to address the false-negative instances (D5). When she returns to the previews view, Divya sees that instances of false negatives have the prompt message she just created. In this manner, Divya can design the interface using direct outputs from the ML model, evaluate her design against different AI and real-world constraints, and iteratively revise the design and repair the API to prototype the AI user experience (D5). 4.5 Implementation We implemented ProtoAI as a web-based application using a clientserver architecture. The server was written in Python and hosts different ML models. We use the metadata format in RunwayML [51] to specify the inputs and outputs to the model. Through this metadata, we generate the client-side model cards. This allows a number of already available models to be used in ProtoAI. Column types are made distinct through colorcoding. The sidebar view shows a model card [50] for each model selected. For example, in a different scenario, Divya can set the data context to all images a persona has taken (e.g., for a photo album interface). From the faceted table, Divya selects the cell value with the persona’s face image and clicks on the ‘Add to Wireframe’ button. Divya also adds the percentage match score value from the Face-ID model’s output to the interface (from design consideration D2). This will allow her to selectively show the explanation overviews later in the previews tab. The widget library can be extended in the future to support additional layout design needs. ProtoAI automatically instantiates the screen interface based on the data context and using all data imported in the data tab (D4). In that case, Divya can configure the data with different cluster sizes and compare the results in the data previews view. 4.4 Analysis, Revision, and Repair 
ProtoAI’s ‘evaluation through previews’ is intended to support the designer in analyzing design breakdowns in differing real-world contexts. In the sidebar, ProtoAI provides a summary of each tag indicating the number of instances with that tag. In this example, Divya sees that the model fails when the person is farther away or when their eyes are closed. The Data tab allows for several different types of data transformations, including the categorization of numerical values (e.g., high, medium, and low), mapping transformations of model-assigned labels and values to end-user-friendly labels, calculating the minimum and maximum values, and custom formula functions for excel-like computation by specifying column headers and cells values to include in the computation (see Figure 3d). Back in the preview mode, the designer sees that cropping fails when there are multiple salient points and no salient points. For example, we imagine a designer using ProtoAI to design a movie recommendation page. The designer then wireframes an initial interface listing all of the movies recommended by the AI. Further by looking at the explanation factors, the designer can incorporate model explanations in text form: “Because you watched [explanation value], we think you might like:” Through data personas with differing inputs, ProtoAI allows designers to simulate model behavior over time of use, and design appropriate interface experiences. If the AI is confident about the end user’s goals, it can tend towards automation. The preview section allows the designer to experience first-hand the subjective utility for end-users and then offer necessary adaptations to their interface design. Six participants had prior experience designing AI-powered applications. Each session lasted 75 minutes, and participants were compensated with $20 for their time. We selected images such that different personas had different score ranges and variations in the differences between scores. Across all sessions, designers created an image recommendation UX with one or more screens (Figure 5). All designers made use of the explanation overlays. For instance, in session 1, the designer created a new task-flow for end-users to crop the image based on salient regions indicated by the heatmap and re-compute the quality score. They all mentioned their current datadriven design workflow either by writing code or analyzing data using spreadsheets. In providing feedback about the overall workflow, P4 commented: “Right now I will have my hypothesis about the data and go back to the engineer and ask them to give me the output, but they say that those data instances will not occur, there is a lack of transparency, and there are layers of gates I need to get through before I can do the next step. They commented they needed scaffolding to understand the AI model and outputs and incorporate data elements in their design. P1 commented that ProtoAI is beneficial at the brainstorming stage, where instead of wireframing on the whiteboard, they can quickly input data and desired model output and test out interface alternatives. Participants rated their learnability (i.e., can learn it quickly) a mean score of 5.33 (SD = 1.65), and learning without written instructions as 3.22 (SD = 1.39). This affords opportunities for communication, negotiation, and co-design between designers and engineers. End-user data is a critical aspect of MIP. Future work should look at ways to support these specific data and analysis needs and advanced user-modeling for MIP.`,
  "introduction-related-work": `When prototyping potential designs for user interfaces (UI), designers work to transform end-user needs into interface specifications [67]. AI-powered applications bring additional challenges to UI prototyping. By exploring AI’s capabilities and limitations through prototyping, they need to design interface adaptations such as explanations for AI outputs, seamless handling of AI failures, and collecting user feedback to improve the AI [4]. By assuming a ‘black-box’ view of AI, tools make it challenging for designers to access necessary AI attributes during the design process [62]. To prototype AI features, designers currently need to work with multiple tools to explore AI behavior (e.g., [51]), probe its capabilities and limitations [24], and evaluate their design with diverse user inputs (e.g., skin-tone, lighting conditions, camera angle, facial features such as beard, glasses, or a mask) [11]. To accomplish this, we propose Model-Informed Prototyping (MIP), a workflow that combines model exploration and interface design tasks. In the data previews tab, ProtoAI automatically generates previews of the interface for each input data, allowing the designer to evaluate the designed AIX for diverse inputs (Figure 1c). MIP streamlines model exploration and UX design tasks during the prototyping process for AI-powered interfaces.`,
  "introduction-design-consideration": `When prototyping potential designs for user interfaces (UI), designers work to transform end-user needs into interface specifications [67]. Thus, designers must identify the interactions between user task-flows and AI capabilities [14, 16, 35] in order to design the user interface for AI experiences (AIX). By exploring AI’s capabilities and limitations through prototyping, they need to design interface adaptations such as explanations for AI outputs, seamless handling of AI failures, and collecting user feedback to improve the AI [4]. To prototype AI features, designers currently need to work with multiple tools to explore AI behavior (e.g., [51]), probe its capabilities and limitations [24], and evaluate their design with diverse user inputs (e.g., skin-tone, lighting conditions, camera angle, facial features such as beard, glasses, or a mask) [11]. In the data previews tab, ProtoAI automatically generates previews of the interface for each input data, allowing the designer to evaluate the designed AIX for diverse inputs (Figure 1c). MIP streamlines model exploration and UX design tasks during the prototyping process for AI-powered interfaces. Standard UI prototyping tools such as Wireframe. This requires a form of vertical prototyping in which designers can access specifications about the underlying AI implementation and map them to AI-powered interfaces [5, 64]. A recommended workflow for UI prototyping consists of three phases: design, test, and analysis. However, the specific dialog in the UI depends on the underlying AI and input data-context. In ProtoAI we take a similar approach and allow designers to incorporate input data and ML model outputs into UI prototypes (e.g., designing password meters by mapping scores from neural networks and heuristics to a visual bar [61]). Further, they should evaluate whether their design is robust to AI’s unpredictability [35]: How does the AI-infused interface react to a diverse set of data and contexts of use [12, 62]? In ProtoAI we automatically generate interface alternatives by invoking built-in models with input data provided by designers. This lets designers experience the UI’s design first hand [9]. Designers need similar analysis and visualization tools at the interface level that will allow them to identify mismatches in model behavior. ProtoAI’s instantiation of UI for different data points allows designers to analyze AI-feature breakdowns without performing mental simulations of differing data contexts.`,
  "introduction-model-informed-prototyping": `When prototyping potential designs for user interfaces (UI), designers work to transform end-user needs into interface specifications [67]. Thus, designers must identify the interactions between user task-flows and AI capabilities [14, 16, 35] in order to design the user interface for AI experiences (AIX). By exploring AI’s capabilities and limitations through prototyping, they need to design interface adaptations such as explanations for AI outputs, seamless handling of AI failures, and collecting user feedback to improve the AI [4]. To prototype AI features, designers currently need to work with multiple tools to explore AI behavior (e.g., [51]), probe its capabilities and limitations [24], and evaluate their design with diverse user inputs (e.g., skin-tone, lighting conditions, camera angle, facial features such as beard, glasses, or a mask) [11]. Through an analysis of current human-AI (HAI) design guidelines from academic and industry sources [4, 23, 37], we identified a set of needs and design considerations for AI prototyping tools. To accomplish this, we propose Model-Informed Prototyping (MIP), a workflow that combines model exploration and interface design tasks. Further, ProtoAI automatically generates data previews of the UI for differing input data, allowing designers to evaluate designs for breakdowns across diverse scenarios and contexts of use. As shown in Figure 1a, to design a Face-ID phone unlock AIX, the designer can begin with a diverse set of registered and new faces along with ground truth data as inputs to the Face Identification model. ProtoAI automatically tags errors such as false positives and false negatives based on ground truth data. MIP streamlines model exploration and UX design tasks during the prototyping process for AI-powered interfaces. Standard UI prototyping tools such as Wireframe. This requires a form of vertical prototyping in which designers can access specifications about the underlying AI implementation and map them to AI-powered interfaces [5, 64]. A recommended workflow for UI prototyping consists of three phases: design, test, and analysis. Further, they should evaluate whether their design is robust to AI’s unpredictability [35]: How does the AI-infused interface react to a diverse set of data and contexts of use [12, 62]? This lets designers experience the UI’s design first hand [9]. For instance, through iterative UI experimentation, Quick Access identified UI needs to offer proactive recommendations [60]. We collected a total of 284 Human-AI design guidelines. D1: Prototyping tools should allow designers to invoke ML models by specifying input data directly. This will allow designers to incorporate AI features into end-user tasks appropriately. For instance, they may want to present confidence in the model’s output or show “why” messages to end-users to design for transparency and trust. D2: Prototyping tools should allow designers to incorporate AI outputs into interface design. D3: Prototyping tools should allowdesigners to shapemodel APIs according to end-user needs. In cases when numeric values are not appropriate, designers should determine appropriate mappings to categorical variables. For example, “while all errors are equal to an ML system, not all errors are equal to all people. [ Third, a data context can be bound to an interface state. Designers should be able to switch between model simulation, design, test, analysis, and revision and repair.`,
  "introduction-design-scenarios": `When prototyping potential designs for user interfaces (UI), designers work to transform end-user needs into interface specifications [67]. However, when prototyping AI-powered applications, such a top-down approach is impractical [71]. AI-powered applications bring additional challenges to UI prototyping. Thus, designers must identify the interactions between user task-flows and AI capabilities [14, 16, 35] in order to design the user interface for AI experiences (AIX). To prototype AI features, designers currently need to work with multiple tools to explore AI behavior (e.g., [51]), probe its capabilities and limitations [24], and evaluate their design with diverse user inputs (e.g., skin-tone, lighting conditions, camera angle, facial features such as beard, glasses, or a mask) [11]. In the data previews tab, ProtoAI automatically generates previews of the interface for each input data, allowing the designer to evaluate the designed AIX for diverse inputs (Figure 1c). MIP streamlines model exploration and UX design tasks during the prototyping process for AI-powered interfaces. Standard UI prototyping tools such as Wireframe. cc [66], Figma [20], and Adobe XD [1] allow designers to work at the user interface level alone through horizontal prototyping [5]. In ProtoAI, our goal is to address this need by designing a vertical prototyping tool for AIpowered interfaces. A recommended workflow for UI prototyping consists of three phases: design, test, and analysis. For instance, in mixed-initiative design, AI systems automatically act on end-users’ goals (when clear) and use interface ‘dialog’ to resolve any uncertainty [36]. In ProtoAI we take a similar approach and allow designers to incorporate input data and ML model outputs into UI prototypes (e.g., designing password meters by mapping scores from neural networks and heuristics to a visual bar [61]). 2.2 Test 
AIX designers need to map AI-to-interface features, identify gulfs of execution and evaluation, and assess visual aesthetics for AI features. The What-if tool [24] allows designers to see the confusion matrix for binary classifiers visually [24]. In this regard, both academic and industry sources have put forth design guidelines about good AIX design [4, 23, 37]. D1: Prototyping tools should allow designers to invoke ML models by specifying input data directly. D2: Prototyping tools should allow designers to incorporate AI outputs into interface design. Based on decisions about AI feature integration into interface design, designers may need to revisit the model inputs and outputs (i.e., the API). For instance, designers may need to split complex outputs and explanations into multiple parts and present them one at a time. However, with AI models, we can personalize the end-user experience to highly specific contexts. D5: Prototyping tools should allow flexibility for designers to incorporate model-related data rapidly and iteratively. Third, a data context can be bound to an interface state. When she returns to the previews view, Divya sees that instances of false negatives have the prompt message she just created. This allows Divya to see the probabilistic nature of the AI’s task-flows to ensure that all users meet the desired goals. 4.5 Implementation We implemented ProtoAI as a web-based application using a clientserver architecture. We use the metadata format in RunwayML [51] to specify the inputs and outputs to the model. Through this metadata, we generate the client-side model cards. This allows a number of already available models to be used in ProtoAI. The client is implemented using HTML and JavaScript. Divya selects the company’s Face-ID model and navigates to the Data tab. The Data tab will allow her to import input data for different personas and scenarios of use. Column types are made distinct through colorcoding. The sidebar view shows a model card [50] for each model selected. The Face-ID model that her engineers have created return additional details: a percentage match score (calculated based on face distances in the face embedding space), an explainable heatmap rendering of the input image [55], and a set of Boolean flags for model features (e.g., whether a face was detected, eyes were closed, etc.). Divya first adds the emergency button by selecting the button element from the UI elements tab. From the faceted table, Divya selects the cell value with the persona’s face image and clicks on the ‘Add to Wireframe’ button. Each widget has a predefined layout and can be bound to selected data along with explanation overlays for designers. ProtoAI automatically instantiates the screen interface based on the data context and using all data imported in the data tab (D4). In a different scenario, to evaluate model functionality over time with model learning, Divya can configure data for different personas by providing different amounts of input data. Using the Data Previews view, Divya can iteratively revise the design to make it robust for a wider variety of users and contexts. Divya can also add her own tags to indicate domain-specific types of breakdowns or repairs. The Data tab allows for several different types of data transformations, including the categorization of numerical values (e.g., high, medium, and low), mapping transformations of model-assigned labels and values to end-user-friendly labels, calculating the minimum and maximum values, and custom formula functions for excel-like computation by specifying column headers and cells values to include in the computation (see Figure 3d).`,
  "introduction-preliminary-evaluation": `When prototyping potential designs for user interfaces (UI), designers work to transform end-user needs into interface specifications [67]. By taking a top-down approach, designers: (1) express user requirements as task-flows; (2) map task items into graphical user interface (GUI) objects; and (3) assess different task-to-GUI mappings against end-user needs to finalize the design [45, 67]. However, when prototyping AI-powered applications, such a top-down approach is impractical [71]. AI-powered applications bring additional challenges to UI prototyping. By exploring AI’s capabilities and limitations through prototyping, they need to design interface adaptations such as explanations for AI outputs, seamless handling of AI failures, and collecting user feedback to improve the AI [4]. To prototype AI features, designers currently need to work with multiple tools to explore AI behavior (e.g., [51]), probe its capabilities and limitations [24], and evaluate their design with diverse user inputs (e.g., skin-tone, lighting conditions, camera angle, facial features such as beard, glasses, or a mask) [11]. Thus, the motivating question for our work is: How might prototyping tools allow designers to directly incorporate target AI features during rapid and iterative prototyping? In our system implementing MIP, ProtoAI, designers can directly run target machine learning models by providing input data and then incorporate the model’s outputs in their UI prototypes. After running the model, the designer can prototype the Face-ID user interface using one of the input faces and corresponding model outputs (Figure 1b). MIP streamlines model exploration and UX design tasks during the prototyping process for AI-powered interfaces. A recommended workflow for UI prototyping consists of three phases: design, test, and analysis. For instance, in mixed-initiative design, AI systems automatically act on end-users’ goals (when clear) and use interface ‘dialog’ to resolve any uncertainty [36]. Mixed-fidelity prototypes [49] could allow designers to incorporate high-fidelity data elements in early-stage prototypes to represent ML’s dependency on data [17]. For instance, Suede implements electronically supportedWoZ testing techniques that generate chat messages using test data [38]. In Topiary [42], designers create a map that models people’s location, which the Wizard uses to update locations during testing. In this regard, both academic and industry sources have put forth design guidelines about good AIX design [4, 23, 37]. We collected a total of 284 Human-AI design guidelines. We conducted inductive in-vivo coding to synthesize the main objectives and tasks for designers and the corresponding AI components necessary to accomplish those tasks. D1: Prototyping tools should allow designers to invoke ML models by specifying input data directly. Based on decisions about AI feature integration into interface design, designers may need to revisit the model inputs and outputs (i.e., the API). This is particularly useful when presenting recommendations along with explanations [23]. D4: Prototyping tools should allow designers to evaluate design choices across diverse users and usage contexts. With conventional applications, design typically converges to a set of standard features across all users. However, with AI models, we can personalize the end-user experience to highly specific contexts. For example, “while all errors are equal to an ML system, not all errors are equal to all people. [ Third, a data context can be bound to an interface state. Designers can evaluate the design for diverse users and contexts by generating interface previews for different data contexts. When prototyping for AI features, tools should allow designers to navigate across this design space flexibly. Based on design considerations, we implemented ProtoAI to prototype AI-powered interfaces for AIX design. We use the metadata format in RunwayML [51] to specify the inputs and outputs to the model. The client is implemented using HTML and JavaScript. Divya selects the company’s Face-ID model and navigates to the Data tab. The sidebar view shows a model card [50] for each model selected. Divya also adds the percentage match score value from the Face-ID model’s output to the interface (from design consideration D2). Based on AIX interface design patterns, ProtoAI implements an initial set of widgets for binding Boolean values to images, categorizing items by tags, and showing ranked order of items. 4.3 Design Evaluation 
At this point, Divya has an initial wireframe of the phone unlock interface designed using the portrait image from a single persona. In a different scenario, to evaluate model functionality over time with model learning, Divya can configure data for different personas by providing different amounts of input data. Using the Data Previews view, Divya can iteratively revise the design to make it robust for a wider variety of users and contexts. By checking the ‘show explanations’ flag in the sidebar, Divya can see the match score element she added in the UI tab. Similarly, Divya can also overlay other model factors and outcome values such as identified facial features or saliency maps to help her understand what the model computed from the image. In this example, Divya sees that the model fails when the person is farther away or when their eyes are closed. The Data tab allows for several different types of data transformations, including the categorization of numerical values (e.g., high, medium, and low), mapping transformations of model-assigned labels and values to end-user-friendly labels, calculating the minimum and maximum values, and custom formula functions for excel-like computation by specifying column headers and cells values to include in the computation (see Figure 3d). To investigate this issue, the designer can overlay the image saliency map (class activation mapping) returned by the model. By tagging the images with appropriate labels, the designer sees that around 30% of images in the dataset have multiple salient points, and 5% have no salient points. To suggest a fix, the designer proposes an image widget for users that pans between different salient points in a loop. ProtoAI can help simulate design previews over time and use. For example, we imagine a designer using ProtoAI to design a movie recommendation page. Further by looking at the explanation factors, the designer can incorporate model explanations in text form: “Because you watched [explanation value], we think you might like:” Through data personas with differing inputs, ProtoAI allows designers to simulate model behavior over time of use, and design appropriate interface experiences. If the AI is confident about the end user’s goals, it can tend towards automation.`,
  "introduction-discussion-and-future-work": `When prototyping potential designs for user interfaces (UI), designers work to transform end-user needs into interface specifications [67]. By assessing those alternatives against user needs (e.g., fast to unlock, secure, low cognitive effort to remember), the designer will finalize the UI design. AI-powered applications bring additional challenges to UI prototyping. Thus, designers must identify the interactions between user task-flows and AI capabilities [14, 16, 35] in order to design the user interface for AI experiences (AIX). In the process, AIX designers also need to assess interface choices against diverse users and contexts of use. By assuming a ‘black-box’ view of AI, tools make it challenging for designers to access necessary AI attributes during the design process [62]. However, without exploring the AI’s behavior first-hand, the designer may not know what inputs the AI needs (e.g., head frontal-view). In our system implementing MIP, ProtoAI, designers can directly run target machine learning models by providing input data and then incorporate the model’s outputs in their UI prototypes. As shown in Figure 1a, to design a Face-ID phone unlock AIX, the designer can begin with a diverse set of registered and new faces along with ground truth data as inputs to the Face Identification model. Standard UI prototyping tools such as Wireframe. This requires a form of vertical prototyping in which designers can access specifications about the underlying AI implementation and map them to AI-powered interfaces [5, 64]. For instance, in mixed-initiative design, AI systems automatically act on end-users’ goals (when clear) and use interface ‘dialog’ to resolve any uncertainty [36]. Wizard of Oz (WoZ) testing is also effective for evaluating early-stage prototypes [8, 18, 48], and a number of data-dependent systems implement digitally scaffolded ‘wizards’ for testing prototypes during design [15, 29, 38, 42]. The What-if tool [24] allows designers to see the confusion matrix for binary classifiers visually [24]. Designers should also be able to incorporate subjective metrics at the intersection of model performance and UX (e.g., subjective perception of errors [39]). We collected a total of 284 Human-AI design guidelines. D2: Prototyping tools should allow designers to incorporate AI outputs into interface design. For instance, designers may need to split complex outputs and explanations into multiple parts and present them one at a time. This is particularly useful when presenting recommendations along with explanations [23]. For example, “while all errors are equal to an ML system, not all errors are equal to all people. [ D5: Prototyping tools should allow flexibility for designers to incorporate model-related data rapidly and iteratively. Third, a data context can be bound to an interface state. Designers can evaluate the design for diverse users and contexts by generating interface previews for different data contexts. ProtoAI consists of four main views: (1) an AI models and services view (these can be implemented AI services or models, or Wizard of Oz ‘stubs’), (2) a data view to import diverse input data for model simulation, (3) a UI ‘designer’ view to visually construct the interface prototype, and (4) a data previews view to simulate the interface design across different input data contexts. After creating the Boolean column, Divya returns to the UI tab to address the false-negative instances (D5). This allows Divya to see the probabilistic nature of the AI’s task-flows to ensure that all users meet the desired goals. 4.5 Implementation We implemented ProtoAI as a web-based application using a clientserver architecture. The client is implemented using HTML and JavaScript. Divya’s company has already assigned an engineering team to the project, and they have been working on an initial version of the Face-ID model. Divya selects the company’s Face-ID model and navigates to the Data tab. The Data tab will allow her to import input data for different personas and scenarios of use. Column types are made distinct through colorcoding. The sidebar view shows a model card [50] for each model selected. Once configured, Divya runs the Face-ID model for the imported data (aligned with design consideration D1). In an alternate scenario, in the absence of a pre-existing model, Divya can use the spreadsheet view to “draft” desired model behavior and outputs for different input data and share those specifications with her engineering team. The Face-ID model that her engineers have created return additional details: a percentage match score (calculated based on face distances in the face embedding space), an explainable heatmap rendering of the input image [55], and a set of Boolean flags for model features (e.g., whether a face was detected, eyes were closed, etc.). This will allow her to selectively show the explanation overviews later in the previews tab. For other complex layout needs, Divya can select entire columns, or brush select the desired data from the data table and add them to the interface as a widget. 4.3 Design Evaluation 
At this point, Divya has an initial wireframe of the phone unlock interface designed using the portrait image from a single persona. In a different scenario, to evaluate model functionality over time with model learning, Divya can configure data for different personas by providing different amounts of input data. 4.4 Analysis, Revision, and Repair 
ProtoAI’s ‘evaluation through previews’ is intended to support the designer in analyzing design breakdowns in differing real-world contexts. In the sidebar, ProtoAI provides a summary of each tag indicating the number of instances with that tag. By checking the ‘show explanations’ flag in the sidebar, Divya can see the match score element she added in the UI tab. Divya can also add her own tags to indicate domain-specific types of breakdowns or repairs. We imagine a process by which ProtoAI can be used to fix the bias in cropping. By tagging the images with appropriate labels, the designer sees that around 30% of images in the dataset have multiple salient points, and 5% have no salient points. For example, we imagine a designer using ProtoAI to design a movie recommendation page. The designer can set up the data for different personas (either real-world preference or simulated)1. By looking at the confidence score for recommendations, the designer creates a new screen state for low confidence recommendations: instead of showing the movies, it asks end-users to select movie genres of interest. We selected images such that different personas had different score ranges and variations in the differences between scores. 6.1 Findings 
6.1.1 Model-Informed Prototyping with ProtoAI. All designers made use of the explanation overlays. By looking at the CAM heat maps, they revised their designs in ways we did not anticipate. In session 5, the designer suggested addressing tasks with no high-quality images by showing the CAM view to end-users and allowing them to retake the photo. A breakdown of individual components show that participants ratings were: mental demand: 59 (SD = 18.07); physical demand: 24 (SD = 21.53); temporal demand:38 (SD = 20.70); performance: 38.5 (SD = 18.86); effort:49 (SD = 12.2); and frustration:37 (SD = 16.36). Designers with prior experience designing AIpowered applications and knowledge of HAI guidelines (n=6) saw value in MIP (and ProtoAI). They all mentioned their current datadriven design workflow either by writing code or analyzing data using spreadsheets. In providing feedback about the overall workflow, P4 commented: “Right now I will have my hypothesis about the data and go back to the engineer and ask them to give me the output, but they say that those data instances will not occur, there is a lack of transparency, and there are layers of gates I need to get through before I can do the next step. You can make something look good with fake labels and ‘ipsum-lorem,’ but using real data to mock things up helps you see where things are broken. Participants rated their learnability (i.e., can learn it quickly) a mean score of 5.33 (SD = 1.65), and learning without written instructions as 3.22 (SD = 1.39). Encouraged by the overall feedback, we plan to conduct a comparative evaluation of ProtoAI against commercial prototyping tools and assess the quality of design output using ProtoAI.`,
  "introduction-conclusion": `When prototyping potential designs for user interfaces (UI), designers work to transform end-user needs into interface specifications [67]. AI-powered applications bring additional challenges to UI prototyping. Thus, designers must identify the interactions between user task-flows and AI capabilities [14, 16, 35] in order to design the user interface for AI experiences (AIX). In the process, AIX designers also need to assess interface choices against diverse users and contexts of use. They may fail to understand how accurately the AI can perform, when it might fail, and how to prompt users experiencing failure (e.g., by asking them to move closer to the camera). To prototype AI features, designers currently need to work with multiple tools to explore AI behavior (e.g., [51]), probe its capabilities and limitations [24], and evaluate their design with diverse user inputs (e.g., skin-tone, lighting conditions, camera angle, facial features such as beard, glasses, or a mask) [11]. Thus, the motivating question for our work is: How might prototyping tools allow designers to directly incorporate target AI features during rapid and iterative prototyping? Through an analysis of current human-AI (HAI) design guidelines from academic and industry sources [4, 23, 37], we identified a set of needs and design considerations for AI prototyping tools. They also need to identify different kinds of interface breakdowns such as mismatch with end-user expectations, low utility (high cost) from using AI, and data specific failures and offer repairs to recover the user experience. In the data previews tab, ProtoAI automatically generates previews of the interface for each input data, allowing the designer to evaluate the designed AIX for diverse inputs (Figure 1c). MIP streamlines model exploration and UX design tasks during the prototyping process for AI-powered interfaces. This requires a form of vertical prototyping in which designers can access specifications about the underlying AI implementation and map them to AI-powered interfaces [5, 64]. A recommended workflow for UI prototyping consists of three phases: design, test, and analysis. Building on existing UX practice, designers may consider approaches such as constructing personas with varying quantitative data [53]. In ProtoAI we automatically generate interface alternatives by invoking built-in models with input data provided by designers. Tools exist for engineers to analyze the overall performance and look at individual data points to reason about model failures (e.g., [3]). The What-if tool [24] allows designers to see the confusion matrix for binary classifiers visually [24]. We collected a total of 284 Human-AI design guidelines. We find that the guidelines offer best-practice recommendations to map AI features into UI design patterns (and end-user tasks). D1: Prototyping tools should allow designers to invoke ML models by specifying input data directly. For instance, they may want to present confidence in the model’s output or show “why” messages to end-users to design for transparency and trust. With conventional applications, design typically converges to a set of standard features across all users. For example, “while all errors are equal to an ML system, not all errors are equal to all people. [ Third, a data context can be bound to an interface state. Designers should be able to switch between model simulation, design, test, analysis, and revision and repair. This allows Divya to see the probabilistic nature of the AI’s task-flows to ensure that all users meet the desired goals. In this manner, Divya can design the interface using direct outputs from the ML model, evaluate her design against different AI and real-world constraints, and iteratively revise the design and repair the API to prototype the AI user experience (D5). Through this metadata, we generate the client-side model cards. The client is implemented using HTML and JavaScript. Divya selects the company’s Face-ID model and navigates to the Data tab. The Data tab will allow her to import input data for different personas and scenarios of use. Column types are made distinct through colorcoding. The sidebar view shows a model card [50] for each model selected. Once configured, Divya runs the Face-ID model for the imported data (aligned with design consideration D1). In an alternate scenario, in the absence of a pre-existing model, Divya can use the spreadsheet view to “draft” desired model behavior and outputs for different input data and share those specifications with her engineering team. The design canvas starts with a default phone template, but Divya can select others if needed (e.g., desktop or tablet). For other complex layout needs, Divya can select entire columns, or brush select the desired data from the data table and add them to the interface as a widget. The widget library can be extended in the future to support additional layout design needs. In a different scenario, to evaluate model functionality over time with model learning, Divya can configure data for different personas by providing different amounts of input data. This will allow Divya to visually see how the AI-powered interface responds after differing degrees of use. 4.4 Analysis, Revision, and Repair 
ProtoAI’s ‘evaluation through previews’ is intended to support the designer in analyzing design breakdowns in differing real-world contexts. Because Divya specified ground truth data for each of the photos, ProtoAI automatically compares the ground truth (Face-ID match) with model predictions and tags instances of false positives and false negatives. By checking the ‘show explanations’ flag in the sidebar, Divya can see the match score element she added in the UI tab. Divya can also add her own tags to indicate domain-specific types of breakdowns or repairs. To address this issue, Divya switches back to the Data tab and creates a new calculated Boolean column that is set to ’true’ if the face is not detected or when eyes are closed (D3). The Data tab allows for several different types of data transformations, including the categorization of numerical values (e.g., high, medium, and low), mapping transformations of model-assigned labels and values to end-user-friendly labels, calculating the minimum and maximum values, and custom formula functions for excel-like computation by specifying column headers and cells values to include in the computation (see Figure 3d). Back in the preview mode, the designer sees that cropping fails when there are multiple salient points and no salient points. ProtoAI can help simulate design previews over time and use. For example, we imagine a designer using ProtoAI to design a movie recommendation page. The designer can set up the data for different personas (either real-world preference or simulated)1. The designer then wireframes an initial interface listing all of the movies recommended by the AI. Further by looking at the explanation factors, the designer can incorporate model explanations in text form: “Because you watched [explanation value], we think you might like:” Through data personas with differing inputs, ProtoAI allows designers to simulate model behavior over time of use, and design appropriate interface experiences. Six participants had prior experience designing AI-powered applications. Each session lasted 75 minutes, and participants were compensated with $20 for their time. We asked participants to think-aloud during this phase of the session and recorded them. Once participants completed the task, we conducted a freeform interview to understand how they would use ProtoAI for AIX problems they had worked on in the past, provided feedback on ProtoAI’s features and interface, and commented on MIP workflow. For instance, in session 1, the designer created a new task-flow for end-users to crop the image based on salient regions indicated by the heatmap and re-compute the quality score. In session 5, the designer suggested addressing tasks with no high-quality images by showing the CAM view to end-users and allowing them to retake the photo. When commenting about prototyping for data instances, P5 commented: “The hardest thing about designing for AI is getting the right data. You can make something look good with fake labels and ‘ipsum-lorem,’ but using real data to mock things up helps you see where things are broken. Participants rated their learnability (i.e., can learn it quickly) a mean score of 5.33 (SD = 1.65), and learning without written instructions as 3.22 (SD = 1.39). To accomplish AIX design, we have demonstrated how ProtoAI’s implementation of Model-Informed Prototyping allows designers to (1) directly incorporate an AI’s output into their design, (2) test their design across different input data contexts, and (3) iteratively assess and adapt their interfaces for explainability, failure, and model feedback. Specifically, future work can investigate how AIX designers can drive AI model parameters based on interface features, negotiate model features and outputs necessary for explainability, and communicate discovered failure instances with engineers for model improvement. End-user data is a critical aspect of MIP. Future work should look at ways to support these specific data and analysis needs and advanced user-modeling for MIP.`,
  "related-work-design-consideration": `The user interface design process consists of a series of transformations between end-user task requirements and the user interface syntax [67]. Standard UI prototyping tools such as Wireframe. This requires a form of vertical prototyping in which designers can access specifications about the underlying AI implementation and map them to AI-powered interfaces [5, 64]. A recommended workflow for UI prototyping consists of three phases: design, test, and analysis. Here,we describe requirements and techniques from prior literature for each phase as applied to AI-powered interfaces. Further, they should evaluate whether their design is robust to AI’s unpredictability [35]: How does the AI-infused interface react to a diverse set of data and contexts of use [12, 62]? Building on existing UX practice, designers may consider approaches such as constructing personas with varying quantitative data [53]. Wizard of Oz (WoZ) testing is also effective for evaluating early-stage prototypes [8, 18, 48], and a number of data-dependent systems implement digitally scaffolded ‘wizards’ for testing prototypes during design [15, 29, 38, 42]. This lets designers experience the UI’s design first hand [9].`,
  "related-work-model-informed-prototyping": `The user interface design process consists of a series of transformations between end-user task requirements and the user interface syntax [67]. Standard UI prototyping tools such as Wireframe. This requires a form of vertical prototyping in which designers can access specifications about the underlying AI implementation and map them to AI-powered interfaces [5, 64]. A recommended workflow for UI prototyping consists of three phases: design, test, and analysis. Mixed-fidelity prototypes [49] could allow designers to incorporate high-fidelity data elements in early-stage prototypes to represent ML’s dependency on data [17]. In Topiary [42], designers create a map that models people’s location, which the Wizard uses to update locations during testing. In ProtoAI we automatically generate interface alternatives by invoking built-in models with input data provided by designers. For instance, D.tools offers a ‘group analysis’ mode aggregating data from multiple user sessions into one view [29]. Designers should also be able to incorporate subjective metrics at the intersection of model performance and UX (e.g., subjective perception of errors [39]). The DECOR system characterizes multi-device responsive UIs as a design repair problem and offers techniques for efficient repairs [57]. In this regard, both academic and industry sources have put forth design guidelines about good AIX design [4, 23, 37]. D1: Prototyping tools should allow designers to invoke ML models by specifying input data directly. D2: Prototyping tools should allow designers to incorporate AI outputs into interface design. This will give designers a more accurate representation than placeholder elements when making design choices (e.g., presentation layout, conditional logic for UI, error presentation, guided-recovery from failure, feedback controls, etc.). For instance, designers may need to split complex outputs and explanations into multiple parts and present them one at a time. With conventional applications, design typically converges to a set of standard features across all users. Third, a data context can be bound to an interface state. When prototyping for AI features, tools should allow designers to navigate across this design space flexibly.`,
  "related-work-design-scenarios": `The user interface design process consists of a series of transformations between end-user task requirements and the user interface syntax [67]. Standard UI prototyping tools such as Wireframe. This requires a form of vertical prototyping in which designers can access specifications about the underlying AI implementation and map them to AI-powered interfaces [5, 64]. A recommended workflow for UI prototyping consists of three phases: design, test, and analysis. Further, given most designers’ limited expertise with AI [70], prototyping tools should make AI features more accessible, immediate (support rapid iterative feedback, reflection-in-action, and reflection-on-action), and generative (allow test, probe, and exploration iterations) for UI designers [13, 27, 41]. In ProtoAI we automatically generate interface alternatives by invoking built-in models with input data provided by designers. Inspired by this approach, in ProtoAI, we allow designers to configure desired behavior (ground truth) by providing model output data for comparison (i.e., designer as wizard [8]). The What-if tool [24] allows designers to see the confusion matrix for binary classifiers visually [24]. For instance, through iterative UI experimentation, Quick Access identified UI needs to offer proactive recommendations [60]. In this regard, both academic and industry sources have put forth design guidelines about good AIX design [4, 23, 37]. We collected a total of 284 Human-AI design guidelines. Further, designers need access to AI model features to offer the rationale behind specific outputs for specific users. D2: Prototyping tools should allow designers to incorporate AI outputs into interface design. For instance, designers may need to split complex outputs and explanations into multiple parts and present them one at a time. When presenting statistical or numeric outputs, the designer needs to consider factors such as precision and rounding. With conventional applications, design typically converges to a set of standard features across all users. For example, “while all errors are equal to an ML system, not all errors are equal to all people. [ D5: Prototyping tools should allow flexibility for designers to incorporate model-related data rapidly and iteratively. Designers can evaluate the design for diverse users and contexts by generating interface previews for different data contexts. Designers should be able to switch between model simulation, design, test, analysis, and revision and repair. When she returns to the previews view, Divya sees that instances of false negatives have the prompt message she just created. 4.5 Implementation We implemented ProtoAI as a web-based application using a clientserver architecture. The server was written in Python and hosts different ML models. Through this metadata, we generate the client-side model cards. This allows a number of already available models to be used in ProtoAI. Column types are made distinct through colorcoding. To simulate the model with this data, Divya maps the column headers on the spreadsheet with the model card inputs by selecting from a dropdown list of all columns. These images become the input data columns. The model output columns are color-coded to match the model configuration card. In an alternate scenario, in the absence of a pre-existing model, Divya can use the spreadsheet view to “draft” desired model behavior and outputs for different input data and share those specifications with her engineering team. The Face-ID model that her engineers have created return additional details: a percentage match score (calculated based on face distances in the face embedding space), an explainable heatmap rendering of the input image [55], and a set of Boolean flags for model features (e.g., whether a face was detected, eyes were closed, etc.). From the faceted table, Divya selects the cell value with the persona’s face image and clicks on the ‘Add to Wireframe’ button. She selects the Data Previews tab to evaluate her current design against different personas and their photos. ProtoAI automatically instantiates the screen interface based on the data context and using all data imported in the data tab (D4). This will allow Divya to visually see how the AI-powered interface responds after differing degrees of use.`,
  "related-work-preliminary-evaluation": `The user interface design process consists of a series of transformations between end-user task requirements and the user interface syntax [67]. Standard UI prototyping tools such as Wireframe. This requires a form of vertical prototyping in which designers can access specifications about the underlying AI implementation and map them to AI-powered interfaces [5, 64]. A recommended workflow for UI prototyping consists of three phases: design, test, and analysis. For instance, in mixed-initiative design, AI systems automatically act on end-users’ goals (when clear) and use interface ‘dialog’ to resolve any uncertainty [36]. Further, given most designers’ limited expertise with AI [70], prototyping tools should make AI features more accessible, immediate (support rapid iterative feedback, reflection-in-action, and reflection-on-action), and generative (allow test, probe, and exploration iterations) for UI designers [13, 27, 41]. For instance, Suede implements electronically supportedWoZ testing techniques that generate chat messages using test data [38]. In ProtoAI we automatically generate interface alternatives by invoking built-in models with input data provided by designers. We collected a total of 284 Human-AI design guidelines. D1: Prototyping tools should allow designers to invoke ML models by specifying input data directly. D2: Prototyping tools should allow designers to incorporate AI outputs into interface design. Based on decisions about AI feature integration into interface design, designers may need to revisit the model inputs and outputs (i.e., the API). For instance, designers may need to split complex outputs and explanations into multiple parts and present them one at a time. With conventional applications, design typically converges to a set of standard features across all users. For example, “while all errors are equal to an ML system, not all errors are equal to all people. [ Based on our analysis of the design guidelines, we formulate a design space for Model-Informed Prototyping. Third, a data context can be bound to an interface state. Designers can evaluate the design for diverse users and contexts by generating interface previews for different data contexts. When prototyping for AI features, tools should allow designers to navigate across this design space flexibly. After creating the Boolean column, Divya returns to the UI tab to address the false-negative instances (D5). When she returns to the previews view, Divya sees that instances of false negatives have the prompt message she just created. In this manner, Divya can design the interface using direct outputs from the ML model, evaluate her design against different AI and real-world constraints, and iteratively revise the design and repair the API to prototype the AI user experience (D5). 4.5 Implementation We implemented ProtoAI as a web-based application using a clientserver architecture. We use the metadata format in RunwayML [51] to specify the inputs and outputs to the model. Through this metadata, we generate the client-side model cards. This allows a number of already available models to be used in ProtoAI. Divya selects the company’s Face-ID model and navigates to the Data tab. Column types are made distinct through colorcoding. These images become the input data columns. The Face-ID model that her engineers have created return additional details: a percentage match score (calculated based on face distances in the face embedding space), an explainable heatmap rendering of the input image [55], and a set of Boolean flags for model features (e.g., whether a face was detected, eyes were closed, etc.). From the faceted table, Divya selects the cell value with the persona’s face image and clicks on the ‘Add to Wireframe’ button. While not intended for the final deliverable, Divya can use it to test and debug the interface design. The widget library can be extended in the future to support additional layout design needs. ProtoAI automatically instantiates the screen interface based on the data context and using all data imported in the data tab (D4). Divya can also add her own tags to indicate domain-specific types of breakdowns or repairs. In this example, Divya sees that the model fails when the person is farther away or when their eyes are closed. The Data tab allows for several different types of data transformations, including the categorization of numerical values (e.g., high, medium, and low), mapping transformations of model-assigned labels and values to end-user-friendly labels, calculating the minimum and maximum values, and custom formula functions for excel-like computation by specifying column headers and cells values to include in the computation (see Figure 3d). By tagging the images with appropriate labels, the designer sees that around 30% of images in the dataset have multiple salient points, and 5% have no salient points. To suggest a fix, the designer proposes an image widget for users that pans between different salient points in a loop. ProtoAI can help simulate design previews over time and use. For example, we imagine a designer using ProtoAI to design a movie recommendation page. The designer can set up the data for different personas (either real-world preference or simulated)1. Further by looking at the explanation factors, the designer can incorporate model explanations in text form: “Because you watched [explanation value], we think you might like:” Through data personas with differing inputs, ProtoAI allows designers to simulate model behavior over time of use, and design appropriate interface experiences.`,
  "related-work-discussion-and-future-work": `The user interface design process consists of a series of transformations between end-user task requirements and the user interface syntax [67]. Standard UI prototyping tools such as Wireframe. For instance, in mixed-initiative design, AI systems automatically act on end-users’ goals (when clear) and use interface ‘dialog’ to resolve any uncertainty [36]. However, the specific dialog in the UI depends on the underlying AI and input data-context. This lets designers experience the UI’s design first hand [9]. Inspired by this approach, in ProtoAI, we allow designers to configure desired behavior (ground truth) by providing model output data for comparison (i.e., designer as wizard [8]). Designers should also be able to incorporate subjective metrics at the intersection of model performance and UX (e.g., subjective perception of errors [39]). In this regard, both academic and industry sources have put forth design guidelines about good AIX design [4, 23, 37]. More importantly, the guidelines prescribe design ‘fixes’ to lower end-user impact from AI-breakdowns such as (1) end-user context breakdown: AI performs poorly for some user-data and in some usage contexts; (2) expectation breakdown: AI behavior and outputs do not align with end-user mental models; and (3) task-utility breakdown: higher cost of using AI due to its failure to understand end-user goals. Further, designers need access to AI model features to offer the rationale behind specific outputs for specific users. To realize these design objectives, designers need to evaluate model performance for potential end-user inputs during their prototyping process. D2: Prototyping tools should allow designers to incorporate AI outputs into interface design. Further, the AIgenerated content should be visually different to allow end-users to adjust their expectations about AI features (and, in turn, diminish frustration). With conventional applications, design typically converges to a set of standard features across all users. D5: Prototyping tools should allow flexibility for designers to incorporate model-related data rapidly and iteratively. ProtoAI consists of four main views: (1) an AI models and services view (these can be implemented AI services or models, or Wizard of Oz ‘stubs’), (2) a data view to import diverse input data for model simulation, (3) a UI ‘designer’ view to visually construct the interface prototype, and (4) a data previews view to simulate the interface design across different input data contexts. After creating the Boolean column, Divya returns to the UI tab to address the false-negative instances (D5). When she returns to the previews view, Divya sees that instances of false negatives have the prompt message she just created. In this manner, Divya can design the interface using direct outputs from the ML model, evaluate her design against different AI and real-world constraints, and iteratively revise the design and repair the API to prototype the AI user experience (D5). 4.5 Implementation We implemented ProtoAI as a web-based application using a clientserver architecture. We use the metadata format in RunwayML [51] to specify the inputs and outputs to the model. Through this metadata, we generate the client-side model cards. This allows a number of already available models to be used in ProtoAI. The client is implemented using HTML and JavaScript. Divya selects the company’s Face-ID model and navigates to the Data tab. The Data tab will allow her to import input data for different personas and scenarios of use. Column types are made distinct through colorcoding. The sidebar view shows a model card [50] for each model selected. Divya first adds the emergency button by selecting the button element from the UI elements tab. The faceting feature is flexible and can set the data context to a single row or a set of rows nested and grouped by column names. For example, in a different scenario, Divya can set the data context to all images a persona has taken (e.g., for a photo album interface). Based on AIX interface design patterns, ProtoAI implements an initial set of widgets for binding Boolean values to images, categorizing items by tags, and showing ranked order of items. She selects the Data Previews tab to evaluate her current design against different personas and their photos. Third, Divya can also evaluate the design for localization by providing inputs in different languages. Because Divya specified ground truth data for each of the photos, ProtoAI automatically compares the ground truth (Face-ID match) with model predictions and tags instances of false positives and false negatives. Similarly, Divya can also overlay other model factors and outcome values such as identified facial features or saliency maps to help her understand what the model computed from the image. Divya can also add her own tags to indicate domain-specific types of breakdowns or repairs. The Data tab allows for several different types of data transformations, including the categorization of numerical values (e.g., high, medium, and low), mapping transformations of model-assigned labels and values to end-user-friendly labels, calculating the minimum and maximum values, and custom formula functions for excel-like computation by specifying column headers and cells values to include in the computation (see Figure 3d). For example, we imagine a designer using ProtoAI to design a movie recommendation page. By looking at the confidence score for recommendations, the designer creates a new screen state for low confidence recommendations: instead of showing the movies, it asks end-users to select movie genres of interest. 5.3 Chat Assistant–Mixed Initiative Design 
A guiding principle for integrating AI capabilities into task workflows is to determine the utility of the AI for end users [36]. Six participants had prior experience designing AI-powered applications. Each session lasted 75 minutes, and participants were compensated with $20 for their time. We selected images such that different personas had different score ranges and variations in the differences between scores. 6.1 Findings 
6.1.1 Model-Informed Prototyping with ProtoAI. In five of the sessions, designers directly started the activity using data elements, including persona images and quality scores. For instance, in session 1, the designer created a new task-flow for end-users to crop the image based on salient regions indicated by the heatmap and re-compute the quality score. They all mentioned their current datadriven design workflow either by writing code or analyzing data using spreadsheets. You can make something look good with fake labels and ‘ipsum-lorem,’ but using real data to mock things up helps you see where things are broken. For participants new to AIX design, they compared MIP to their current workflows. P1 commented that ProtoAI is beneficial at the brainstorming stage, where instead of wireframing on the whiteboard, they can quickly input data and desired model output and test out interface alternatives. Participants rated their learnability (i.e., can learn it quickly) a mean score of 5.33 (SD = 1.65), and learning without written instructions as 3.22 (SD = 1.39).`,
  "related-work-conclusion": `The user interface design process consists of a series of transformations between end-user task requirements and the user interface syntax [67]. Standard UI prototyping tools such as Wireframe. This requires a form of vertical prototyping in which designers can access specifications about the underlying AI implementation and map them to AI-powered interfaces [5, 64]. A recommended workflow for UI prototyping consists of three phases: design, test, and analysis. Further, given most designers’ limited expertise with AI [70], prototyping tools should make AI features more accessible, immediate (support rapid iterative feedback, reflection-in-action, and reflection-on-action), and generative (allow test, probe, and exploration iterations) for UI designers [13, 27, 41]. In Topiary [42], designers create a map that models people’s location, which the Wizard uses to update locations during testing. Inspired by this approach, in ProtoAI, we allow designers to configure desired behavior (ground truth) by providing model output data for comparison (i.e., designer as wizard [8]). The What-if tool [24] allows designers to see the confusion matrix for binary classifiers visually [24]. For instance, through iterative UI experimentation, Quick Access identified UI needs to offer proactive recommendations [60]. We collected a total of 284 Human-AI design guidelines. D1: Prototyping tools should allow designers to invoke ML models by specifying input data directly. For instance, they may want to present confidence in the model’s output or show “why” messages to end-users to design for transparency and trust. D2: Prototyping tools should allow designers to incorporate AI outputs into interface design. Further, the AIgenerated content should be visually different to allow end-users to adjust their expectations about AI features (and, in turn, diminish frustration). For instance, designers may need to split complex outputs and explanations into multiple parts and present them one at a time. With conventional applications, design typically converges to a set of standard features across all users. For example, “while all errors are equal to an ML system, not all errors are equal to all people. [ Third, a data context can be bound to an interface state. When prototyping for AI features, tools should allow designers to navigate across this design space flexibly. ProtoAI consists of four main views: (1) an AI models and services view (these can be implemented AI services or models, or Wizard of Oz ‘stubs’), (2) a data view to import diverse input data for model simulation, (3) a UI ‘designer’ view to visually construct the interface prototype, and (4) a data previews view to simulate the interface design across different input data contexts. This allows Divya to see the probabilistic nature of the AI’s task-flows to ensure that all users meet the desired goals. 4.5 Implementation We implemented ProtoAI as a web-based application using a clientserver architecture. This allows a number of already available models to be used in ProtoAI. The client is implemented using HTML and JavaScript. Column types are made distinct through colorcoding. The sidebar view shows a model card [50] for each model selected. To simulate the model with this data, Divya maps the column headers on the spreadsheet with the model card inputs by selecting from a dropdown list of all columns. The model output columns are color-coded to match the model configuration card. In an alternate scenario, in the absence of a pre-existing model, Divya can use the spreadsheet view to “draft” desired model behavior and outputs for different input data and share those specifications with her engineering team. 4.2 User Interface 
Design Divya selects the User Interface tab, which consists of a design canvas and a sidebar for interface elements. From the faceted table, Divya selects the cell value with the persona’s face image and clicks on the ‘Add to Wireframe’ button. This will allow her to selectively show the explanation overviews later in the previews tab. For other complex layout needs, Divya can select entire columns, or brush select the desired data from the data table and add them to the interface as a widget. The widget library can be extended in the future to support additional layout design needs. Third, Divya can also evaluate the design for localization by providing inputs in different languages. Divya can also add her own tags to indicate domain-specific types of breakdowns or repairs. The Data tab allows for several different types of data transformations, including the categorization of numerical values (e.g., high, medium, and low), mapping transformations of model-assigned labels and values to end-user-friendly labels, calculating the minimum and maximum values, and custom formula functions for excel-like computation by specifying column headers and cells values to include in the computation (see Figure 3d). Through these transformations, Divya can design the model’s API. By tagging the images with appropriate labels, the designer sees that around 30% of images in the dataset have multiple salient points, and 5% have no salient points. For example, we imagine a designer using ProtoAI to design a movie recommendation page. The designer then wireframes an initial interface listing all of the movies recommended by the AI. By looking at the confidence score for recommendations, the designer creates a new screen state for low confidence recommendations: instead of showing the movies, it asks end-users to select movie genres of interest. Further by looking at the explanation factors, the designer can incorporate model explanations in text form: “Because you watched [explanation value], we think you might like:” Through data personas with differing inputs, ProtoAI allows designers to simulate model behavior over time of use, and design appropriate interface experiences. If the AI is confident about the end user’s goals, it can tend towards automation. Six participants had prior experience designing AI-powered applications. Each session lasted 75 minutes, and participants were compensated with $20 for their time. 6.1 Findings 
6.1.1 Model-Informed Prototyping with ProtoAI. All designers made use of the explanation overlays. In session 5, the designer suggested addressing tasks with no high-quality images by showing the CAM view to end-users and allowing them to retake the photo. They all mentioned their current datadriven design workflow either by writing code or analyzing data using spreadsheets. In providing feedback about the overall workflow, P4 commented: “Right now I will have my hypothesis about the data and go back to the engineer and ask them to give me the output, but they say that those data instances will not occur, there is a lack of transparency, and there are layers of gates I need to get through before I can do the next step. This tool makes it easy for me to carry out the entire flow on my own.” Overall, participants found ProtoAI’s interface intuitive and easy to use. Participants rated their learnability (i.e., can learn it quickly) a mean score of 5.33 (SD = 1.65), and learning without written instructions as 3.22 (SD = 1.39). Encouraged by the overall feedback, we plan to conduct a comparative evaluation of ProtoAI against commercial prototyping tools and assess the quality of design output using ProtoAI. End-user data is a critical aspect of MIP. In this regard, ProtoAI offers flexibility for designers to manually input data from user research and simulated data to explore design their hypotheses about AI behavior. Future work should look at ways to support these specific data and analysis needs and advanced user-modeling for MIP. This allows us to trade-off design complexity for detailed data.`,
  "design-consideration-model-informed-prototyping": `A primary objective when prototyping AIX is to maximize endusers’ success. More importantly, the guidelines prescribe design ‘fixes’ to lower end-user impact from AI-breakdowns such as (1) end-user context breakdown: AI performs poorly for some user-data and in some usage contexts; (2) expectation breakdown: AI behavior and outputs do not align with end-user mental models; and (3) task-utility breakdown: higher cost of using AI due to its failure to understand end-user goals. On this basis, we derived a set of design considerations for AIX prototyping tools (i.e., model-informed prototyping). For instance, designers may need to split complex outputs and explanations into multiple parts and present them one at a time. With conventional applications, design typically converges to a set of standard features across all users. However, with AI models, we can personalize the end-user experience to highly specific contexts. For example, “while all errors are equal to an ML system, not all errors are equal to all people. [ D5: Prototyping tools should allow flexibility for designers to incorporate model-related data rapidly and iteratively. Third, a data context can be bound to an interface state. Designers should be able to switch between model simulation, design, test, analysis, and revision and repair.`,
  "design-consideration-design-scenarios": `A primary objective when prototyping AIX is to maximize endusers’ success. We find that the guidelines offer best-practice recommendations to map AI features into UI design patterns (and end-user tasks). D1: Prototyping tools should allow designers to invoke ML models by specifying input data directly. In mixed-initiative design, designers need to find the right presentation based on confidence thresholds (e.g., showing only the high accuracy item, ranking items as a list, etc.). Based on decisions about AI feature integration into interface design, designers may need to revisit the model inputs and outputs (i.e., the API). Design guidelines recommend that AI model APIs be designed based on principles of information architecture for interface design. For instance, designers may need to split complex outputs and explanations into multiple parts and present them one at a time. For example, “while all errors are equal to an ML system, not all errors are equal to all people. [ Third, a data context can be bound to an interface state. When prototyping for AI features, tools should allow designers to navigate across this design space flexibly. Designers should be able to switch between model simulation, design, test, analysis, and revision and repair. After creating the Boolean column, Divya returns to the UI tab to address the false-negative instances (D5). This allows Divya to see the probabilistic nature of the AI’s task-flows to ensure that all users meet the desired goals. 4.5 Implementation We implemented ProtoAI as a web-based application using a clientserver architecture. Through this metadata, we generate the client-side model cards. This allows a number of already available models to be used in ProtoAI. The client is implemented using HTML and JavaScript. Divya’s company has already assigned an engineering team to the project, and they have been working on an initial version of the Face-ID model. Column types are made distinct through colorcoding. The model output columns are color-coded to match the model configuration card. To design the phone unlock experience, Divya wants to show the camera view in full screen, along with a button at the bottom for emergency calls and an icon on top to indicate the phone is scanning for a face. This panel consists of a faceting control to set the wireframe’s data context and a table showing the faceted data itself (a subset of the main spreadsheet view). This will allow Divya to visually see how the AI-powered interface responds after differing degrees of use. In that case, Divya can configure the data with different cluster sizes and compare the results in the data previews view. Using the Data Previews view, Divya can iteratively revise the design to make it robust for a wider variety of users and contexts. By checking the ‘show explanations’ flag in the sidebar, Divya can see the match score element she added in the UI tab. The Data tab allows for several different types of data transformations, including the categorization of numerical values (e.g., high, medium, and low), mapping transformations of model-assigned labels and values to end-user-friendly labels, calculating the minimum and maximum values, and custom formula functions for excel-like computation by specifying column headers and cells values to include in the computation (see Figure 3d).`,
  "design-consideration-preliminary-evaluation": `A primary objective when prototyping AIX is to maximize endusers’ success. We collected a total of 284 Human-AI design guidelines. To address these breakdowns, designers need access to the underlying AI model, features, and output data for diverse end-user inputs. D1: Prototyping tools should allow designers to invoke ML models by specifying input data directly. This will allow designers to incorporate AI features into end-user tasks appropriately. For instance, they may want to present confidence in the model’s output or show “why” messages to end-users to design for transparency and trust. D2: Prototyping tools should allow designers to incorporate AI outputs into interface design. In mixed-initiative design, designers need to find the right presentation based on confidence thresholds (e.g., showing only the high accuracy item, ranking items as a list, etc.). Design guidelines recommend that AI model APIs be designed based on principles of information architecture for interface design. With conventional applications, design typically converges to a set of standard features across all users. For example, “while all errors are equal to an ML system, not all errors are equal to all people. [ When prototyping for AI features, tools should allow designers to navigate across this design space flexibly. Designers should be able to switch between model simulation, design, test, analysis, and revision and repair. Based on design considerations, we implemented ProtoAI to prototype AI-powered interfaces for AIX design. In ProtoAI, each screen can be assigned different screen states dependent on model behavior and values. In this state, Divya adds a message at the top of the screen prompting the user to move closer to the screen. This allows Divya to see the probabilistic nature of the AI’s task-flows to ensure that all users meet the desired goals. This allows a number of already available models to be used in ProtoAI. The client is implemented using HTML and JavaScript. The Data tab will allow her to import input data for different personas and scenarios of use. Column types are made distinct through colorcoding. The sidebar view shows a model card [50] for each model selected. Based on her user research, Divya has curated a set of personas and portrait photos for each persona taken across different usage context (e.g., low light condition, crowded subway, person with a beard, facial hair, different skin tones, etc. ). In an alternate scenario, in the absence of a pre-existing model, Divya can use the spreadsheet view to “draft” desired model behavior and outputs for different input data and share those specifications with her engineering team. From the faceted table, Divya selects the cell value with the persona’s face image and clicks on the ‘Add to Wireframe’ button. 4.3 Design Evaluation 
At this point, Divya has an initial wireframe of the phone unlock interface designed using the portrait image from a single persona. Because Divya specified ground truth data for each of the photos, ProtoAI automatically compares the ground truth (Face-ID match) with model predictions and tags instances of false positives and false negatives. By checking the ‘show explanations’ flag in the sidebar, Divya can see the match score element she added in the UI tab. The Data tab allows for several different types of data transformations, including the categorization of numerical values (e.g., high, medium, and low), mapping transformations of model-assigned labels and values to end-user-friendly labels, calculating the minimum and maximum values, and custom formula functions for excel-like computation by specifying column headers and cells values to include in the computation (see Figure 3d). We imagine a process by which ProtoAI can be used to fix the bias in cropping. They curate this data based on user research on photos uploaded to social media. To investigate this issue, the designer can overlay the image saliency map (class activation mapping) returned by the model. To suggest a fix, the designer proposes an image widget for users that pans between different salient points in a loop. For example, we imagine a designer using ProtoAI to design a movie recommendation page. The designer then wireframes an initial interface listing all of the movies recommended by the AI.`,
  "design-consideration-discussion-and-future-work": `A primary objective when prototyping AIX is to maximize endusers’ success. We collected a total of 284 Human-AI design guidelines. To make these choices, designers need to understand how the AI performs for different input data, what output it returns, and under what conditions it might fail. D2: Prototyping tools should allow designers to incorporate AI outputs into interface design. With conventional applications, design typically converges to a set of standard features across all users. However, with AI models, we can personalize the end-user experience to highly specific contexts. For example, “while all errors are equal to an ML system, not all errors are equal to all people. [ 23]” To operationalize these recommendations, designers need to evaluate their interface choices across diverse data. D5: Prototyping tools should allow flexibility for designers to incorporate model-related data rapidly and iteratively. Designers can evaluate the design for diverse users and contexts by generating interface previews for different data contexts. Designers should be able to switch between model simulation, design, test, analysis, and revision and repair. ProtoAI consists of four main views: (1) an AI models and services view (these can be implemented AI services or models, or Wizard of Oz ‘stubs’), (2) a data view to import diverse input data for model simulation, (3) a UI ‘designer’ view to visually construct the interface prototype, and (4) a data previews view to simulate the interface design across different input data contexts. After creating the Boolean column, Divya returns to the UI tab to address the false-negative instances (D5). In this manner, Divya can design the interface using direct outputs from the ML model, evaluate her design against different AI and real-world constraints, and iteratively revise the design and repair the API to prototype the AI user experience (D5). The server was written in Python and hosts different ML models. We use the metadata format in RunwayML [51] to specify the inputs and outputs to the model. This allows a number of already available models to be used in ProtoAI. Column types are made distinct through colorcoding. The sidebar view shows a model card [50] for each model selected. In an alternate scenario, in the absence of a pre-existing model, Divya can use the spreadsheet view to “draft” desired model behavior and outputs for different input data and share those specifications with her engineering team. The Face-ID model that her engineers have created return additional details: a percentage match score (calculated based on face distances in the face embedding space), an explainable heatmap rendering of the input image [55], and a set of Boolean flags for model features (e.g., whether a face was detected, eyes were closed, etc.). The faceting feature is flexible and can set the data context to a single row or a set of rows nested and grouped by column names. Divya also adds the percentage match score value from the Face-ID model’s output to the interface (from design consideration D2). To indicate this to ProtoAI, Divya toggles the ’set as explanation’ flag in the properties tab for the score element. This will allow her to selectively show the explanation overviews later in the previews tab. Based on AIX interface design patterns, ProtoAI implements an initial set of widgets for binding Boolean values to images, categorizing items by tags, and showing ranked order of items. The widget library can be extended in the future to support additional layout design needs. Divya can also check her design for different data sizes from model output (e.g., recommendations), ranging from no recommendations, a few recommendations, to tens of recommendations. In this example, Divya sees that the model fails when the person is farther away or when their eyes are closed. Through these transformations, Divya can design the model’s API. They curate this data based on user research on photos uploaded to social media. By tagging the images with appropriate labels, the designer sees that around 30% of images in the dataset have multiple salient points, and 5% have no salient points. 5.3 Chat Assistant–Mixed Initiative Design 
A guiding principle for integrating AI capabilities into task workflows is to determine the utility of the AI for end users [36]. If the AI is confident about the end user’s goals, it can tend towards automation. Six participants had prior experience designing AI-powered applications. In the remaining five sessions, designers first created placeholder layouts and then imported data from the data elements tab. All designers made use of the explanation overlays. For instance, in session 1, the designer created a new task-flow for end-users to crop the image based on salient regions indicated by the heatmap and re-compute the quality score. Based on the NASA-TLX questionnaire, participants rated the overall task workload of 50.3 (SD = 12.49). They all mentioned their current datadriven design workflow either by writing code or analyzing data using spreadsheets. In providing feedback about the overall workflow, P4 commented: “Right now I will have my hypothesis about the data and go back to the engineer and ask them to give me the output, but they say that those data instances will not occur, there is a lack of transparency, and there are layers of gates I need to get through before I can do the next step. You can make something look good with fake labels and ‘ipsum-lorem,’ but using real data to mock things up helps you see where things are broken. For participants new to AIX design, they compared MIP to their current workflows. They commented they needed scaffolding to understand the AI model and outputs and incorporate data elements in their design. Based on the usability questionnaire, on a seven-point scale, participants rated ProtoAI’s to be easy to use (mean = 5.88,SD = 0.9), and flexible (mean = 5.63,SD = 0.72).`,
  "design-consideration-conclusion": `A primary objective when prototyping AIX is to maximize endusers’ success. We collected a total of 284 Human-AI design guidelines. We find that the guidelines offer best-practice recommendations to map AI features into UI design patterns (and end-user tasks). To address these breakdowns, designers need access to the underlying AI model, features, and output data for diverse end-user inputs. D1: Prototyping tools should allow designers to invoke ML models by specifying input data directly. D2: Prototyping tools should allow designers to incorporate AI outputs into interface design. For instance, designers may need to split complex outputs and explanations into multiple parts and present them one at a time. With conventional applications, design typically converges to a set of standard features across all users. For example, “while all errors are equal to an ML system, not all errors are equal to all people. [ D5: Prototyping tools should allow flexibility for designers to incorporate model-related data rapidly and iteratively. Based on our analysis of the design guidelines, we formulate a design space for Model-Informed Prototyping. Third, a data context can be bound to an interface state. Designers should be able to switch between model simulation, design, test, analysis, and revision and repair. After creating the Boolean column, Divya returns to the UI tab to address the false-negative instances (D5). This allows a number of already available models to be used in ProtoAI. The client is implemented using HTML and JavaScript. Divya selects the company’s Face-ID model and navigates to the Data tab. The Data tab will allow her to import input data for different personas and scenarios of use. Column types are made distinct through colorcoding. The model output columns are color-coded to match the model configuration card. In an alternate scenario, in the absence of a pre-existing model, Divya can use the spreadsheet view to “draft” desired model behavior and outputs for different input data and share those specifications with her engineering team. To design the phone unlock experience, Divya wants to show the camera view in full screen, along with a button at the bottom for emergency calls and an icon on top to indicate the phone is scanning for a face. The data context is the scope of end-user data that will be bound to the interface at runtime. For example, in a different scenario, Divya can set the data context to all images a persona has taken (e.g., for a photo album interface). She selects the Data Previews tab to evaluate her current design against different personas and their photos. This will allow Divya to visually see how the AI-powered interface responds after differing degrees of use. In that case, Divya can configure the data with different cluster sizes and compare the results in the data previews view. The Data tab allows for several different types of data transformations, including the categorization of numerical values (e.g., high, medium, and low), mapping transformations of model-assigned labels and values to end-user-friendly labels, calculating the minimum and maximum values, and custom formula functions for excel-like computation by specifying column headers and cells values to include in the computation (see Figure 3d). Through these transformations, Divya can design the model’s API. By tagging the images with appropriate labels, the designer sees that around 30% of images in the dataset have multiple salient points, and 5% have no salient points. For example, we imagine a designer using ProtoAI to design a movie recommendation page. The designer then wireframes an initial interface listing all of the movies recommended by the AI. By looking at the confidence score for recommendations, the designer creates a new screen state for low confidence recommendations: instead of showing the movies, it asks end-users to select movie genres of interest. Further by looking at the explanation factors, the designer can incorporate model explanations in text form: “Because you watched [explanation value], we think you might like:” Through data personas with differing inputs, ProtoAI allows designers to simulate model behavior over time of use, and design appropriate interface experiences. If the AI is confident about the end user’s goals, it can tend towards automation. In ProtoAI, the designer can curate such chat messages for the ML model. We selected images such that different personas had different score ranges and variations in the differences between scores. For each image, we also generated a class activation heatmap [55] to probe participants on the explainability features of ProtoAI. In session 5, the designer suggested addressing tasks with no high-quality images by showing the CAM view to end-users and allowing them to retake the photo. They all mentioned their current datadriven design workflow either by writing code or analyzing data using spreadsheets. Overall, participants found ProtoAI’s interface intuitive and easy to use. They appreciated the flexibility and connectedness of end-user data across different tabs (Data, UI, and Previews). P1 commented that ProtoAI is beneficial at the brainstorming stage, where instead of wireframing on the whiteboard, they can quickly input data and desired model output and test out interface alternatives. Participants rated their learnability (i.e., can learn it quickly) a mean score of 5.33 (SD = 1.65), and learning without written instructions as 3.22 (SD = 1.39). To design user interfaces for AI-powered applications, designers need access to the underlying AI. In this regard, ProtoAI offers flexibility for designers to manually input data from user research and simulated data to explore design their hypotheses about AI behavior. AI engineers are asked to evaluate their data and ML models for responsible AI criteria (e.g., AI Fairness 360 [6]), and AIX designers can use tools like ProtoAI’s data previews to detect interface failures in responsible AI design. This allows us to trade-off design complexity for detailed data. Finally, as pedagogy and practice of AI application design continues to evolve, we envision AIX tools like ProtoAI will enable students and novice designers to develop necessary skills for AIX prototyping. We imagine a library of widgets implementing AIX design patterns and explainable overlays to scaffold designers’ learning process.`,
  "model-informed-prototyping-design-scenarios": `Based on design considerations, we implemented ProtoAI to prototype AI-powered interfaces for AIX design. After creating the Boolean column, Divya returns to the UI tab to address the false-negative instances (D5). The server was written in Python and hosts different ML models. We use the metadata format in RunwayML [51] to specify the inputs and outputs to the model. Through this metadata, we generate the client-side model cards. Divya selects the company’s Face-ID model and navigates to the Data tab. The Data tab will allow her to import input data for different personas and scenarios of use. Column types are made distinct through colorcoding. To simulate the model with this data, Divya maps the column headers on the spreadsheet with the model card inputs by selecting from a dropdown list of all columns. Once configured, Divya runs the Face-ID model for the imported data (aligned with design consideration D1). The model output columns are color-coded to match the model configuration card. 4.2 User Interface 
Design Divya selects the User Interface tab, which consists of a design canvas and a sidebar for interface elements. Next, to engage in design-by-instance prototyping, Divya opens the data elements panel (Figure 3c). From the faceted table, Divya selects the cell value with the persona’s face image and clicks on the ‘Add to Wireframe’ button. For other complex layout needs, Divya can select entire columns, or brush select the desired data from the data table and add them to the interface as a widget. Third, Divya can also evaluate the design for localization by providing inputs in different languages. Using the Data Previews view, Divya can iteratively revise the design to make it robust for a wider variety of users and contexts. By checking the ‘show explanations’ flag in the sidebar, Divya can see the match score element she added in the UI tab.`,
  "model-informed-prototyping-preliminary-evaluation": `Based on design considerations, we implemented ProtoAI to prototype AI-powered interfaces for AIX design. After creating the Boolean column, Divya returns to the UI tab to address the false-negative instances (D5). 4.5 Implementation We implemented ProtoAI as a web-based application using a clientserver architecture. We use the metadata format in RunwayML [51] to specify the inputs and outputs to the model. Through this metadata, we generate the client-side model cards. This allows a number of already available models to be used in ProtoAI. The client is implemented using HTML and JavaScript. The Data tab will allow her to import input data for different personas and scenarios of use. Column types are made distinct through colorcoding. The sidebar view shows a model card [50] for each model selected. From the Face-ID model card, Divya sees that the model requires images (both for training/registration) and optional ground truth labels. In an alternate scenario, in the absence of a pre-existing model, Divya can use the spreadsheet view to “draft” desired model behavior and outputs for different input data and share those specifications with her engineering team. The Face-ID model that her engineers have created return additional details: a percentage match score (calculated based on face distances in the face embedding space), an explainable heatmap rendering of the input image [55], and a set of Boolean flags for model features (e.g., whether a face was detected, eyes were closed, etc.). This panel consists of a faceting control to set the wireframe’s data context and a table showing the faceted data itself (a subset of the main spreadsheet view). 4.3 Design Evaluation 
At this point, Divya has an initial wireframe of the phone unlock interface designed using the portrait image from a single persona. She selects the Data Previews tab to evaluate her current design against different personas and their photos. ProtoAI automatically instantiates the screen interface based on the data context and using all data imported in the data tab (D4). As shown in Figure 1c, the Data Previews tab consists of a scrolling grid view of the UI rendered for different users and their portrait photo variations. By checking the ‘show explanations’ flag in the sidebar, Divya can see the match score element she added in the UI tab. Divya can also add her own tags to indicate domain-specific types of breakdowns or repairs. In this example, Divya sees that the model fails when the person is farther away or when their eyes are closed. The Data tab allows for several different types of data transformations, including the categorization of numerical values (e.g., high, medium, and low), mapping transformations of model-assigned labels and values to end-user-friendly labels, calculating the minimum and maximum values, and custom formula functions for excel-like computation by specifying column headers and cells values to include in the computation (see Figure 3d). For example, we imagine a designer using ProtoAI to design a movie recommendation page. Further by looking at the explanation factors, the designer can incorporate model explanations in text form: “Because you watched [explanation value], we think you might like:” Through data personas with differing inputs, ProtoAI allows designers to simulate model behavior over time of use, and design appropriate interface experiences. If the AI is confident about the end user’s goals, it can tend towards automation.`,
  "model-informed-prototyping-discussion-and-future-work": `Based on design considerations, we implemented ProtoAI to prototype AI-powered interfaces for AIX design. ProtoAI consists of four main views: (1) an AI models and services view (these can be implemented AI services or models, or Wizard of Oz ‘stubs’), (2) a data view to import diverse input data for model simulation, (3) a UI ‘designer’ view to visually construct the interface prototype, and (4) a data previews view to simulate the interface design across different input data contexts. This allows Divya to see the probabilistic nature of the AI’s task-flows to ensure that all users meet the desired goals. The server was written in Python and hosts different ML models. We use the metadata format in RunwayML [51] to specify the inputs and outputs to the model. Divya selects the company’s Face-ID model and navigates to the Data tab. The Data tab will allow her to import input data for different personas and scenarios of use. Column types are made distinct through colorcoding. The sidebar view shows a model card [50] for each model selected. The Face-ID model that her engineers have created return additional details: a percentage match score (calculated based on face distances in the face embedding space), an explainable heatmap rendering of the input image [55], and a set of Boolean flags for model features (e.g., whether a face was detected, eyes were closed, etc.). Divya first adds the emergency button by selecting the button element from the UI elements tab. For other complex layout needs, Divya can select entire columns, or brush select the desired data from the data table and add them to the interface as a widget. Third, Divya can also evaluate the design for localization by providing inputs in different languages. 4.4 Analysis, Revision, and Repair 
ProtoAI’s ‘evaluation through previews’ is intended to support the designer in analyzing design breakdowns in differing real-world contexts. By checking the ‘show explanations’ flag in the sidebar, Divya can see the match score element she added in the UI tab. Divya can also add her own tags to indicate domain-specific types of breakdowns or repairs. To address this issue, Divya switches back to the Data tab and creates a new calculated Boolean column that is set to ’true’ if the face is not detected or when eyes are closed (D3). Through these transformations, Divya can design the model’s API. To resolve images with no salient points, the designer adds interface functionality for manual cropping using the AI-generated crop region as a suggestion. ProtoAI can help simulate design previews over time and use. The designer can set up the data for different personas (either real-world preference or simulated)1. Six participants had prior experience designing AI-powered applications. Each session lasted 75 minutes, and participants were compensated with $20 for their time. We selected images such that different personas had different score ranges and variations in the differences between scores. At the end of the study, participants filled out a usability questionnaire [44] and the NASA-Task Load Index questionnaire [28]. In five of the sessions, designers directly started the activity using data elements, including persona images and quality scores. All designers made use of the explanation overlays. For instance, in session 1, the designer created a new task-flow for end-users to crop the image based on salient regions indicated by the heatmap and re-compute the quality score. Designers with prior experience designing AIpowered applications and knowledge of HAI guidelines (n=6) saw value in MIP (and ProtoAI). They all mentioned their current datadriven design workflow either by writing code or analyzing data using spreadsheets. In providing feedback about the overall workflow, P4 commented: “Right now I will have my hypothesis about the data and go back to the engineer and ask them to give me the output, but they say that those data instances will not occur, there is a lack of transparency, and there are layers of gates I need to get through before I can do the next step. This tool makes it easy for me to carry out the entire flow on my own.” Overall, participants found ProtoAI’s interface intuitive and easy to use. P1 commented that ProtoAI is beneficial at the brainstorming stage, where instead of wireframing on the whiteboard, they can quickly input data and desired model output and test out interface alternatives. Based on the usability questionnaire, on a seven-point scale, participants rated ProtoAI’s to be easy to use (mean = 5.88,SD = 0.9), and flexible (mean = 5.63,SD = 0.72). Encouraged by the overall feedback, we plan to conduct a comparative evaluation of ProtoAI against commercial prototyping tools and assess the quality of design output using ProtoAI.`,
  "model-informed-prototyping-conclusion": `Based on design considerations, we implemented ProtoAI to prototype AI-powered interfaces for AIX design. After creating the Boolean column, Divya returns to the UI tab to address the false-negative instances (D5). In this manner, Divya can design the interface using direct outputs from the ML model, evaluate her design against different AI and real-world constraints, and iteratively revise the design and repair the API to prototype the AI user experience (D5). 4.5 Implementation We implemented ProtoAI as a web-based application using a clientserver architecture. Through this metadata, we generate the client-side model cards. This allows a number of already available models to be used in ProtoAI. The client is implemented using HTML and JavaScript. Divya selects the company’s Face-ID model and navigates to the Data tab. Column types are made distinct through colorcoding. The sidebar view shows a model card [50] for each model selected. Based on her user research, Divya has curated a set of personas and portrait photos for each persona taken across different usage context (e.g., low light condition, crowded subway, person with a beard, facial hair, different skin tones, etc. ). The model output columns are color-coded to match the model configuration card. This will allow her to selectively show the explanation overviews later in the previews tab. ProtoAI automatically instantiates the screen interface based on the data context and using all data imported in the data tab (D4). By checking the ‘show explanations’ flag in the sidebar, Divya can see the match score element she added in the UI tab. Similarly, Divya can also overlay other model factors and outcome values such as identified facial features or saliency maps to help her understand what the model computed from the image. In this example, Divya sees that the model fails when the person is farther away or when their eyes are closed. The Data tab allows for several different types of data transformations, including the categorization of numerical values (e.g., high, medium, and low), mapping transformations of model-assigned labels and values to end-user-friendly labels, calculating the minimum and maximum values, and custom formula functions for excel-like computation by specifying column headers and cells values to include in the computation (see Figure 3d). By tagging the images with appropriate labels, the designer sees that around 30% of images in the dataset have multiple salient points, and 5% have no salient points. ProtoAI can help simulate design previews over time and use. Later, they can create different screen-states for mixed-initiative and manual inputs using confidence score thresholds. For each image, we also generated a class activation heatmap [55] to probe participants on the explainability features of ProtoAI. 6.1 Findings 
6.1.1 Model-Informed Prototyping with ProtoAI. For instance, in session 10, the designer crafted an initial layout showing only the image with the highest score. For instance, in session 1, the designer created a new task-flow for end-users to crop the image based on salient regions indicated by the heatmap and re-compute the quality score. Based on the NASA-TLX questionnaire, participants rated the overall task workload of 50.3 (SD = 12.49). Designers with prior experience designing AIpowered applications and knowledge of HAI guidelines (n=6) saw value in MIP (and ProtoAI). This tool makes it easy for me to carry out the entire flow on my own.” When commenting about prototyping for data instances, P5 commented: “The hardest thing about designing for AI is getting the right data. You can make something look good with fake labels and ‘ipsum-lorem,’ but using real data to mock things up helps you see where things are broken. Overall, participants found ProtoAI’s interface intuitive and easy to use. P1 commented that ProtoAI is beneficial at the brainstorming stage, where instead of wireframing on the whiteboard, they can quickly input data and desired model output and test out interface alternatives. Based on the usability questionnaire, on a seven-point scale, participants rated ProtoAI’s to be easy to use (mean = 5.88,SD = 0.9), and flexible (mean = 5.63,SD = 0.72). To accomplish AIX design, we have demonstrated how ProtoAI’s implementation of Model-Informed Prototyping allows designers to (1) directly incorporate an AI’s output into their design, (2) test their design across different input data contexts, and (3) iteratively assess and adapt their interfaces for explainability, failure, and model feedback. In addition, the data-level representations in ProtoAI correspond to engineering representations of the AI service’s API. End-user data is a critical aspect of MIP. In this regard, ProtoAI offers flexibility for designers to manually input data from user research and simulated data to explore design their hypotheses about AI behavior. We are currently exploring ways to support synthetic data generation needs through expressive queries. This allows us to trade-off design complexity for detailed data. This includes supporting interactive and click-through prototypes, sharing prototypes with end-users, and logging capabilities. Finally, as pedagogy and practice of AI application design continues to evolve, we envision AIX tools like ProtoAI will enable students and novice designers to develop necessary skills for AIX prototyping.`,
  "design-scenarios-preliminary-evaluation": `To demonstrate the utility of ProtoAI in operationalizing HAI design guidelines, we offer three usage scenarios based on real-world examples. By tagging the images with appropriate labels, the designer sees that around 30% of images in the dataset have multiple salient points, and 5% have no salient points. ProtoAI can help simulate design previews over time and use. For example, we imagine a designer using ProtoAI to design a movie recommendation page. By looking at the confidence score for recommendations, the designer creates a new screen state for low confidence recommendations: instead of showing the movies, it asks end-users to select movie genres of interest. 5.3 Chat Assistant–Mixed Initiative Design 
A guiding principle for integrating AI capabilities into task workflows is to determine the utility of the AI for end users [36]. If the goal can be resolved with minimal support from users, the AI can engage in a mixed-initiative dialog with end-users. In ProtoAI, the designer can curate such chat messages for the ML model.`,
  "design-scenarios-discussion-and-future-work": `To demonstrate the utility of ProtoAI in operationalizing HAI design guidelines, we offer three usage scenarios based on real-world examples. A recent example of this need is the image auto-cropping feature offered on social media feeds (e.g., Twitter’s problem [33] and their response [43]). The designer then wireframes an initial interface listing all of the movies recommended by the AI. Further by looking at the explanation factors, the designer can incorporate model explanations in text form: “Because you watched [explanation value], we think you might like:” Through data personas with differing inputs, ProtoAI allows designers to simulate model behavior over time of use, and design appropriate interface experiences. 5.3 Chat Assistant–Mixed Initiative Design 
A guiding principle for integrating AI capabilities into task workflows is to determine the utility of the AI for end users [36]. For instance, consider a chat assistant’s design that prompts end-users with task actions based on chat messages. In ProtoAI, the designer can curate such chat messages for the ML model. Six participants had prior experience designing AI-powered applications. Once participants completed the task, we conducted a freeform interview to understand how they would use ProtoAI for AIX problems they had worked on in the past, provided feedback on ProtoAI’s features and interface, and commented on MIP workflow. 6.1 Findings 
6.1.1 Model-Informed Prototyping with ProtoAI. In five of the sessions, designers directly started the activity using data elements, including persona images and quality scores. They then created two new derived columns to compute the differences in scores and created new screen states for the best two and three images. For instance, in session 1, the designer created a new task-flow for end-users to crop the image based on salient regions indicated by the heatmap and re-compute the quality score. Based on the NASA-TLX questionnaire, participants rated the overall task workload of 50.3 (SD = 12.49). They all mentioned their current datadriven design workflow either by writing code or analyzing data using spreadsheets. In particular, designers appreciated the data-tointerface pipeline through model simulation, auto-generated data previews, and carrying out data transformations during the design process. They commented they needed scaffolding to understand the AI model and outputs and incorporate data elements in their design. Based on the usability questionnaire, on a seven-point scale, participants rated ProtoAI’s to be easy to use (mean = 5.88,SD = 0.9), and flexible (mean = 5.63,SD = 0.72).`,
  "design-scenarios-conclusion": `To demonstrate the utility of ProtoAI in operationalizing HAI design guidelines, we offer three usage scenarios based on real-world examples. 5.1 Social Media Feed–Automated Image Cropping 
ProtoAI supports testing for, detecting, and fixing context breakdowns during design. Back in the preview mode, the designer sees that cropping fails when there are multiple salient points and no salient points. For example, we imagine a designer using ProtoAI to design a movie recommendation page. The previews tab shows that for personas with few or no input data about movies already watched, the recommendations do not align with the fictional persona’s preferences. 5.3 Chat Assistant–Mixed Initiative Design 
A guiding principle for integrating AI capabilities into task workflows is to determine the utility of the AI for end users [36]. If the AI is confident about the end user’s goals, it can tend towards automation. Six participants had prior experience designing AI-powered applications. Across all sessions, designers created an image recommendation UX with one or more screens (Figure 5). In five of the sessions, designers directly started the activity using data elements, including persona images and quality scores. Designers also used the ’categorize transformation’ function to bucket image scores into high, medium, and low categories. For instance, in session 1, the designer created a new task-flow for end-users to crop the image based on salient regions indicated by the heatmap and re-compute the quality score. They all mentioned their current datadriven design workflow either by writing code or analyzing data using spreadsheets. In providing feedback about the overall workflow, P4 commented: “Right now I will have my hypothesis about the data and go back to the engineer and ask them to give me the output, but they say that those data instances will not occur, there is a lack of transparency, and there are layers of gates I need to get through before I can do the next step. You can make something look good with fake labels and ‘ipsum-lorem,’ but using real data to mock things up helps you see where things are broken. They appreciated the flexibility and connectedness of end-user data across different tabs (Data, UI, and Previews). Participants rated their learnability (i.e., can learn it quickly) a mean score of 5.33 (SD = 1.65), and learning without written instructions as 3.22 (SD = 1.39). Encouraged by the overall feedback, we plan to conduct a comparative evaluation of ProtoAI against commercial prototyping tools and assess the quality of design output using ProtoAI. To design user interfaces for AI-powered applications, designers need access to the underlying AI. To accomplish AIX design, we have demonstrated how ProtoAI’s implementation of Model-Informed Prototyping allows designers to (1) directly incorporate an AI’s output into their design, (2) test their design across different input data contexts, and (3) iteratively assess and adapt their interfaces for explainability, failure, and model feedback. In addition, the data-level representations in ProtoAI correspond to engineering representations of the AI service’s API. End-user data is a critical aspect of MIP. In this regard, ProtoAI offers flexibility for designers to manually input data from user research and simulated data to explore design their hypotheses about AI behavior. Future work should look at ways to support these specific data and analysis needs and advanced user-modeling for MIP.`,
  "preliminary-evaluation-discussion-and-future-work": `To gather feedback on ProtoAI’s implementation of MIP, we conducted a preliminary online user study. Six participants had prior experience designing AI-powered applications. We asked participants to think-aloud during this phase of the session and recorded them. Once participants completed the task, we conducted a freeform interview to understand how they would use ProtoAI for AIX problems they had worked on in the past, provided feedback on ProtoAI’s features and interface, and commented on MIP workflow. Then, by looking at the data tab, they realized there were some small differences between images with the second and third highest scores. For instance, in session 1, the designer created a new task-flow for end-users to crop the image based on salient regions indicated by the heatmap and re-compute the quality score. They commented they needed scaffolding to understand the AI model and outputs and incorporate data elements in their design. P1 commented that ProtoAI is beneficial at the brainstorming stage, where instead of wireframing on the whiteboard, they can quickly input data and desired model output and test out interface alternatives. Participants made suggestions for section and tab labels, which we incorporated into the final design (e.g., in the prototype used in the study, ’data previews’ tab was labeled ‘alternatives’). Participants rated their learnability (i.e., can learn it quickly) a mean score of 5.33 (SD = 1.65), and learning without written instructions as 3.22 (SD = 1.39). Encouraged by the overall feedback, we plan to conduct a comparative evaluation of ProtoAI against commercial prototyping tools and assess the quality of design output using ProtoAI.`,
  "preliminary-evaluation-conclusion": `To gather feedback on ProtoAI’s implementation of MIP, we conducted a preliminary online user study. Six participants had prior experience designing AI-powered applications. Across all sessions, designers created an image recommendation UX with one or more screens (Figure 5). In five of the sessions, designers directly started the activity using data elements, including persona images and quality scores. Designers also used the ’categorize transformation’ function to bucket image scores into high, medium, and low categories. For instance, in session 1, the designer created a new task-flow for end-users to crop the image based on salient regions indicated by the heatmap and re-compute the quality score. Designers with prior experience designing AIpowered applications and knowledge of HAI guidelines (n=6) saw value in MIP (and ProtoAI). In providing feedback about the overall workflow, P4 commented: “Right now I will have my hypothesis about the data and go back to the engineer and ask them to give me the output, but they say that those data instances will not occur, there is a lack of transparency, and there are layers of gates I need to get through before I can do the next step. This tool makes it easy for me to carry out the entire flow on my own.” Participants rated their learnability (i.e., can learn it quickly) a mean score of 5.33 (SD = 1.65), and learning without written instructions as 3.22 (SD = 1.39). To design user interfaces for AI-powered applications, designers need access to the underlying AI. To accomplish AIX design, we have demonstrated how ProtoAI’s implementation of Model-Informed Prototyping allows designers to (1) directly incorporate an AI’s output into their design, (2) test their design across different input data contexts, and (3) iteratively assess and adapt their interfaces for explainability, failure, and model feedback. End-user data is a critical aspect of MIP. In this regard, ProtoAI offers flexibility for designers to manually input data from user research and simulated data to explore design their hypotheses about AI behavior. Besides, they can directly import data from other human-centered design processes (e.g., Data-Assisted Affinity Diagramming [58]). Future work should look at ways to support these specific data and analysis needs and advanced user-modeling for MIP.`,
  "discussion-and-future-work-conclusion": `To design user interfaces for AI-powered applications, designers need access to the underlying AI. In addition, the data-level representations in ProtoAI correspond to engineering representations of the AI service’s API. This affords opportunities for communication, negotiation, and co-design between designers and engineers. In this regard, ProtoAI offers flexibility for designers to manually input data from user research and simulated data to explore design their hypotheses about AI behavior. Future work should look at ways to support these specific data and analysis needs and advanced user-modeling for MIP. AI engineers are asked to evaluate their data and ML models for responsible AI criteria (e.g., AI Fairness 360 [6]), and AIX designers can use tools like ProtoAI’s data previews to detect interface failures in responsible AI design.`,
};

exports.getData = catchAsync(async (req, res, next) => {
  res.status(200).json({
    status: "success",
    data,
    sectionsData,
  });
});
